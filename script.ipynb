{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b1246c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-01 10:27:05,799 - INFO - D√©marrage du scraping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-01 10:27:05,801 - INFO - Chargement de: https://insd.bf\n",
      "2025-11-01 10:27:06,841 - ERROR - √âchec du chargement https://insd.bf: HTTPSConnectionPool(host='insd.bf', port=443): Max retries exceeded with url: / (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000029102AA7250>: Failed to resolve 'insd.bf' ([Errno 11001] getaddrinfo failed)\"))\n",
      "2025-11-01 10:27:09,843 - INFO - Chargement de: https://insd.bf\n",
      "2025-11-01 10:27:09,849 - ERROR - √âchec du chargement https://insd.bf: HTTPSConnectionPool(host='insd.bf', port=443): Max retries exceeded with url: / (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000029102AA79D0>: Failed to resolve 'insd.bf' ([Errno 11001] getaddrinfo failed)\"))\n",
      "2025-11-01 10:27:12,851 - INFO - Chargement de: https://insd.bf\n",
      "2025-11-01 10:27:13,190 - ERROR - √âchec du chargement https://insd.bf: HTTPSConnectionPool(host='insd.bf', port=443): Max retries exceeded with url: / (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000029102AA7D90>: Failed to resolve 'insd.bf' ([Errno 11001] getaddrinfo failed)\"))\n",
      "2025-11-01 10:27:16,192 - INFO - Chargement de: https://insd.bf\n",
      "2025-11-01 10:27:16,199 - ERROR - √âchec du chargement https://insd.bf: HTTPSConnectionPool(host='insd.bf', port=443): Max retries exceeded with url: / (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000029102CDC190>: Failed to resolve 'insd.bf' ([Errno 11001] getaddrinfo failed)\"))\n",
      "2025-11-01 10:27:16,202 - INFO - Chargement de: https://insd.bf\n",
      "2025-11-01 10:27:16,208 - ERROR - √âchec du chargement https://insd.bf: HTTPSConnectionPool(host='insd.bf', port=443): Max retries exceeded with url: / (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000029102CDC550>: Failed to resolve 'insd.bf' ([Errno 11001] getaddrinfo failed)\"))\n",
      "2025-11-01 10:27:19,211 - INFO - Chargement de: https://insd.bf\n",
      "2025-11-01 10:27:19,231 - ERROR - √âchec du chargement https://insd.bf: HTTPSConnectionPool(host='insd.bf', port=443): Max retries exceeded with url: / (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000029102CDC910>: Failed to resolve 'insd.bf' ([Errno 11001] getaddrinfo failed)\"))\n",
      "2025-11-01 10:27:22,234 - INFO - Chargement de: https://insd.bf\n",
      "2025-11-01 10:27:22,245 - ERROR - √âchec du chargement https://insd.bf: HTTPSConnectionPool(host='insd.bf', port=443): Max retries exceeded with url: / (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000029102CDCCD0>: Failed to resolve 'insd.bf' ([Errno 11001] getaddrinfo failed)\"))\n",
      "2025-11-01 10:27:25,246 - INFO - Chargement de: https://insd.bf\n",
      "2025-11-01 10:27:25,304 - ERROR - √âchec du chargement https://insd.bf: HTTPSConnectionPool(host='insd.bf', port=443): Max retries exceeded with url: / (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000029102CDD090>: Failed to resolve 'insd.bf' ([Errno 11001] getaddrinfo failed)\"))\n",
      "2025-11-01 10:27:26,807 - INFO - Chargement de: https://insd.bf/fr/definitions-concept\n",
      "2025-11-01 10:27:26,825 - ERROR - √âchec du chargement https://insd.bf/fr/definitions-concept: HTTPSConnectionPool(host='insd.bf', port=443): Max retries exceeded with url: /fr/definitions-concept (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000029102CDD450>: Failed to resolve 'insd.bf' ([Errno 11001] getaddrinfo failed)\"))\n",
      "2025-11-01 10:27:29,831 - INFO - Chargement de: https://insd.bf/fr/definitions-concept\n",
      "2025-11-01 10:27:30,061 - ERROR - √âchec du chargement https://insd.bf/fr/definitions-concept: HTTPSConnectionPool(host='insd.bf', port=443): Max retries exceeded with url: /fr/definitions-concept (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000029102CDD950>: Failed to resolve 'insd.bf' ([Errno 11001] getaddrinfo failed)\"))\n",
      "2025-11-01 10:27:33,064 - INFO - Chargement de: https://insd.bf/fr/definitions-concept\n",
      "2025-11-01 10:27:33,075 - ERROR - √âchec du chargement https://insd.bf/fr/definitions-concept: HTTPSConnectionPool(host='insd.bf', port=443): Max retries exceeded with url: /fr/definitions-concept (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000029102CDDE50>: Failed to resolve 'insd.bf' ([Errno 11001] getaddrinfo failed)\"))\n",
      "2025-11-01 10:27:36,079 - INFO - Chargement de: https://insd.bf/fr/definitions-concept\n",
      "2025-11-01 10:27:37,255 - ERROR - √âchec du chargement https://insd.bf/fr/definitions-concept: HTTPSConnectionPool(host='insd.bf', port=443): Max retries exceeded with url: /fr/definitions-concept (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000029102CDE350>: Failed to resolve 'insd.bf' ([Errno 11001] getaddrinfo failed)\"))\n",
      "2025-11-01 10:27:37,257 - INFO - Chargement de: https://insd.bf/fr/definitions-concept\n",
      "2025-11-01 10:27:37,264 - ERROR - √âchec du chargement https://insd.bf/fr/definitions-concept: HTTPSConnectionPool(host='insd.bf', port=443): Max retries exceeded with url: /fr/definitions-concept (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000029102CDE5D0>: Failed to resolve 'insd.bf' ([Errno 11001] getaddrinfo failed)\"))\n",
      "2025-11-01 10:27:40,266 - INFO - Chargement de: https://insd.bf/fr/definitions-concept\n",
      "2025-11-01 10:27:40,291 - ERROR - √âchec du chargement https://insd.bf/fr/definitions-concept: HTTPSConnectionPool(host='insd.bf', port=443): Max retries exceeded with url: /fr/definitions-concept (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000029102CDEC10>: Failed to resolve 'insd.bf' ([Errno 11001] getaddrinfo failed)\"))\n",
      "2025-11-01 10:27:43,293 - INFO - Chargement de: https://insd.bf/fr/definitions-concept\n",
      "2025-11-01 10:27:43,514 - ERROR - √âchec du chargement https://insd.bf/fr/definitions-concept: HTTPSConnectionPool(host='insd.bf', port=443): Max retries exceeded with url: /fr/definitions-concept (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000029102CDF110>: Failed to resolve 'insd.bf' ([Errno 11001] getaddrinfo failed)\"))\n",
      "2025-11-01 10:27:46,519 - INFO - Chargement de: https://insd.bf/fr/definitions-concept\n",
      "2025-11-01 10:27:46,526 - ERROR - √âchec du chargement https://insd.bf/fr/definitions-concept: HTTPSConnectionPool(host='insd.bf', port=443): Max retries exceeded with url: /fr/definitions-concept (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000029102CDF610>: Failed to resolve 'insd.bf' ([Errno 11001] getaddrinfo failed)\"))\n",
      "2025-11-01 10:27:48,032 - INFO - Chargement de: https://insd.bf/fr/methodes\n",
      "2025-11-01 10:27:48,038 - ERROR - √âchec du chargement https://insd.bf/fr/methodes: HTTPSConnectionPool(host='insd.bf', port=443): Max retries exceeded with url: /fr/methodes (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000029102CDF890>: Failed to resolve 'insd.bf' ([Errno 11001] getaddrinfo failed)\"))\n",
      "2025-11-01 10:27:51,042 - INFO - Chargement de: https://insd.bf/fr/methodes\n",
      "2025-11-01 10:27:51,068 - ERROR - √âchec du chargement https://insd.bf/fr/methodes: HTTPSConnectionPool(host='insd.bf', port=443): Max retries exceeded with url: /fr/methodes (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000029102CDFD90>: Failed to resolve 'insd.bf' ([Errno 11001] getaddrinfo failed)\"))\n",
      "2025-11-01 10:27:54,070 - INFO - Chargement de: https://insd.bf/fr/methodes\n",
      "2025-11-01 10:27:54,080 - ERROR - √âchec du chargement https://insd.bf/fr/methodes: HTTPSConnectionPool(host='insd.bf', port=443): Max retries exceeded with url: /fr/methodes (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000029103000190>: Failed to resolve 'insd.bf' ([Errno 11001] getaddrinfo failed)\"))\n",
      "2025-11-01 10:27:57,082 - INFO - Chargement de: https://insd.bf/fr/methodes\n",
      "2025-11-01 10:27:57,327 - ERROR - √âchec du chargement https://insd.bf/fr/methodes: HTTPSConnectionPool(host='insd.bf', port=443): Max retries exceeded with url: /fr/methodes (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000029103000550>: Failed to resolve 'insd.bf' ([Errno 11001] getaddrinfo failed)\"))\n",
      "2025-11-01 10:27:57,329 - INFO - Chargement de: https://insd.bf/fr/methodes\n",
      "2025-11-01 10:27:57,335 - ERROR - √âchec du chargement https://insd.bf/fr/methodes: HTTPSConnectionPool(host='insd.bf', port=443): Max retries exceeded with url: /fr/methodes (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000029103000910>: Failed to resolve 'insd.bf' ([Errno 11001] getaddrinfo failed)\"))\n",
      "2025-11-01 10:28:00,337 - INFO - Chargement de: https://insd.bf/fr/methodes\n",
      "2025-11-01 10:28:00,343 - ERROR - √âchec du chargement https://insd.bf/fr/methodes: HTTPSConnectionPool(host='insd.bf', port=443): Max retries exceeded with url: /fr/methodes (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000029103000CD0>: Failed to resolve 'insd.bf' ([Errno 11001] getaddrinfo failed)\"))\n",
      "2025-11-01 10:28:03,346 - INFO - Chargement de: https://insd.bf/fr/methodes\n",
      "2025-11-01 10:28:04,411 - ERROR - √âchec du chargement https://insd.bf/fr/methodes: HTTPSConnectionPool(host='insd.bf', port=443): Max retries exceeded with url: /fr/methodes (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000029103001090>: Failed to resolve 'insd.bf' ([Errno 11001] getaddrinfo failed)\"))\n",
      "2025-11-01 10:28:07,414 - INFO - Chargement de: https://insd.bf/fr/methodes\n",
      "2025-11-01 10:28:07,421 - ERROR - √âchec du chargement https://insd.bf/fr/methodes: HTTPSConnectionPool(host='insd.bf', port=443): Max retries exceeded with url: /fr/methodes (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000029103001450>: Failed to resolve 'insd.bf' ([Errno 11001] getaddrinfo failed)\"))\n",
      "2025-11-01 10:28:08,925 - INFO - Chargement de: https://insd.bf/fr/resultats\n",
      "2025-11-01 10:28:10,125 - ERROR - √âchec du chargement https://insd.bf/fr/resultats: HTTPSConnectionPool(host='insd.bf', port=443): Max retries exceeded with url: /fr/resultats (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000029103001810>: Failed to resolve 'insd.bf' ([Errno 11001] getaddrinfo failed)\"))\n",
      "2025-11-01 10:28:13,128 - INFO - Chargement de: https://insd.bf/fr/resultats\n",
      "2025-11-01 10:28:13,157 - ERROR - √âchec du chargement https://insd.bf/fr/resultats: HTTPSConnectionPool(host='insd.bf', port=443): Max retries exceeded with url: /fr/resultats (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000029103001BD0>: Failed to resolve 'insd.bf' ([Errno 11001] getaddrinfo failed)\"))\n",
      "2025-11-01 10:28:16,161 - INFO - Chargement de: https://insd.bf/fr/resultats\n",
      "2025-11-01 10:28:16,191 - ERROR - √âchec du chargement https://insd.bf/fr/resultats: HTTPSConnectionPool(host='insd.bf', port=443): Max retries exceeded with url: /fr/resultats (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000029102AA7ED0>: Failed to resolve 'insd.bf' ([Errno 11001] getaddrinfo failed)\"))\n",
      "2025-11-01 10:28:19,194 - INFO - Chargement de: https://insd.bf/fr/resultats\n",
      "2025-11-01 10:28:19,201 - ERROR - √âchec du chargement https://insd.bf/fr/resultats: HTTPSConnectionPool(host='insd.bf', port=443): Max retries exceeded with url: /fr/resultats (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000029102AA7B10>: Failed to resolve 'insd.bf' ([Errno 11001] getaddrinfo failed)\"))\n",
      "2025-11-01 10:28:19,205 - INFO - Chargement de: https://insd.bf/fr/resultats\n",
      "2025-11-01 10:28:19,213 - ERROR - √âchec du chargement https://insd.bf/fr/resultats: HTTPSConnectionPool(host='insd.bf', port=443): Max retries exceeded with url: /fr/resultats (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000029102AA7390>: Failed to resolve 'insd.bf' ([Errno 11001] getaddrinfo failed)\"))\n",
      "2025-11-01 10:28:22,217 - INFO - Chargement de: https://insd.bf/fr/resultats\n",
      "2025-11-01 10:28:22,276 - ERROR - √âchec du chargement https://insd.bf/fr/resultats: HTTPSConnectionPool(host='insd.bf', port=443): Max retries exceeded with url: /fr/resultats (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000029103001590>: Failed to resolve 'insd.bf' ([Errno 11001] getaddrinfo failed)\"))\n",
      "2025-11-01 10:28:25,279 - INFO - Chargement de: https://insd.bf/fr/resultats\n",
      "2025-11-01 10:28:25,297 - ERROR - √âchec du chargement https://insd.bf/fr/resultats: HTTPSConnectionPool(host='insd.bf', port=443): Max retries exceeded with url: /fr/resultats (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x00000291030011D0>: Failed to resolve 'insd.bf' ([Errno 11001] getaddrinfo failed)\"))\n",
      "2025-11-01 10:28:28,300 - INFO - Chargement de: https://insd.bf/fr/resultats\n",
      "2025-11-01 10:28:28,494 - ERROR - √âchec du chargement https://insd.bf/fr/resultats: HTTPSConnectionPool(host='insd.bf', port=443): Max retries exceeded with url: /fr/resultats (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000029103000E10>: Failed to resolve 'insd.bf' ([Errno 11001] getaddrinfo failed)\"))\n",
      "2025-11-01 10:28:29,996 - INFO - Chargement de: https://insd.bf/fr/data\n",
      "2025-11-01 10:28:30,017 - ERROR - √âchec du chargement https://insd.bf/fr/data: HTTPSConnectionPool(host='insd.bf', port=443): Max retries exceeded with url: /fr/data (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000029103000A50>: Failed to resolve 'insd.bf' ([Errno 11001] getaddrinfo failed)\"))\n",
      "2025-11-01 10:28:33,020 - INFO - Chargement de: https://insd.bf/fr/data\n",
      "2025-11-01 10:28:33,028 - ERROR - √âchec du chargement https://insd.bf/fr/data: HTTPSConnectionPool(host='insd.bf', port=443): Max retries exceeded with url: /fr/data (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000029103000690>: Failed to resolve 'insd.bf' ([Errno 11001] getaddrinfo failed)\"))\n",
      "2025-11-01 10:28:36,031 - INFO - Chargement de: https://insd.bf/fr/data\n",
      "2025-11-01 10:28:36,171 - ERROR - √âchec du chargement https://insd.bf/fr/data: HTTPSConnectionPool(host='insd.bf', port=443): Max retries exceeded with url: /fr/data (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x00000291030002D0>: Failed to resolve 'insd.bf' ([Errno 11001] getaddrinfo failed)\"))\n",
      "2025-11-01 10:28:39,176 - INFO - Chargement de: https://insd.bf/fr/data\n",
      "2025-11-01 10:28:39,183 - ERROR - √âchec du chargement https://insd.bf/fr/data: HTTPSConnectionPool(host='insd.bf', port=443): Max retries exceeded with url: /fr/data (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000029103001F90>: Failed to resolve 'insd.bf' ([Errno 11001] getaddrinfo failed)\"))\n",
      "2025-11-01 10:28:39,186 - INFO - Chargement de: https://insd.bf/fr/data\n",
      "2025-11-01 10:28:39,194 - ERROR - √âchec du chargement https://insd.bf/fr/data: HTTPSConnectionPool(host='insd.bf', port=443): Max retries exceeded with url: /fr/data (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000029103002350>: Failed to resolve 'insd.bf' ([Errno 11001] getaddrinfo failed)\"))\n",
      "2025-11-01 10:28:42,197 - INFO - Chargement de: https://insd.bf/fr/data\n",
      "2025-11-01 10:28:42,418 - ERROR - √âchec du chargement https://insd.bf/fr/data: HTTPSConnectionPool(host='insd.bf', port=443): Max retries exceeded with url: /fr/data (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000029103002710>: Failed to resolve 'insd.bf' ([Errno 11001] getaddrinfo failed)\"))\n",
      "2025-11-01 10:28:45,422 - INFO - Chargement de: https://insd.bf/fr/data\n",
      "2025-11-01 10:28:45,444 - ERROR - √âchec du chargement https://insd.bf/fr/data: HTTPSConnectionPool(host='insd.bf', port=443): Max retries exceeded with url: /fr/data (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000029103002AD0>: Failed to resolve 'insd.bf' ([Errno 11001] getaddrinfo failed)\"))\n",
      "2025-11-01 10:28:48,447 - INFO - Chargement de: https://insd.bf/fr/data\n",
      "2025-11-01 10:28:48,469 - ERROR - √âchec du chargement https://insd.bf/fr/data: HTTPSConnectionPool(host='insd.bf', port=443): Max retries exceeded with url: /fr/data (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000029103002E90>: Failed to resolve 'insd.bf' ([Errno 11001] getaddrinfo failed)\"))\n",
      "2025-11-01 10:28:49,972 - INFO - Scraping termin√©. Total: 0 enregistrements\n",
      "2025-11-01 10:28:49,974 - WARNING - Aucune donn√©e collect√©e\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Aucune donn√©e extraite\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Script am√©lior√© de collecte de donn√©es - INSD Burkina Faso\n",
    "Collecte des indicateurs statistiques depuis burkinafaso.opendataforafrica.org\n",
    "\"\"\"\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import logging\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from typing import List, Dict, Set, Optional\n",
    "from datetime import datetime\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Configuration du logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(f'scraping_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Configuration\n",
    "class Config:\n",
    "    BASE_URL = \"https://www.insd.bf/sites/default/files/2024-08/\"\n",
    "    ENTRY_POINTS = [\n",
    "        \"\",\n",
    "        \"fr/definitions-concept\",\n",
    "        \"fr/methodes\",\n",
    "        \"fr/resultats\",\n",
    "        \"fr/data\",\n",
    "    ]\n",
    "    HEADERS = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (compatible; DataScraper/1.0; +https://example.org/bot)\",\n",
    "        \"Accept-Language\": \"fr-FR,fr;q=0.9\",\n",
    "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
    "    }\n",
    "    TIMEOUT = 20\n",
    "    DELAY_BETWEEN_REQUESTS = 1.5\n",
    "    MAX_RETRIES = 3\n",
    "    OUTPUT_DIR = Path(\"output\")\n",
    "    \n",
    "\n",
    "class DataScraper:\n",
    "    \"\"\"Classe principale pour le scraping de donn√©es INSD\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Config = Config()):\n",
    "        self.config = config\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update(config.HEADERS)\n",
    "        self.visited_urls: Set[str] = set()\n",
    "        self.all_records: List[Dict] = []\n",
    "        \n",
    "        # Cr√©er le dossier de sortie\n",
    "        self.config.OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "    \n",
    "    def fetch_page(self, url: str, retry_count: int = 0) -> Optional[str]:\n",
    "        \"\"\"\n",
    "        R√©cup√®re le contenu HTML d'une page avec gestion des erreurs et retry\n",
    "        \"\"\"\n",
    "        if url in self.visited_urls:\n",
    "            logger.debug(f\"URL d√©j√† visit√©e: {url}\")\n",
    "            return None\n",
    "            \n",
    "        try:\n",
    "            logger.info(f\"Chargement de: {url}\")\n",
    "            resp = self.session.get(url, timeout=self.config.TIMEOUT)\n",
    "            resp.raise_for_status()\n",
    "            self.visited_urls.add(url)\n",
    "            return resp.text\n",
    "            \n",
    "        except requests.Timeout:\n",
    "            logger.warning(f\"Timeout pour {url}\")\n",
    "            if retry_count < self.config.MAX_RETRIES:\n",
    "                time.sleep(self.config.DELAY_BETWEEN_REQUESTS * 2)\n",
    "                return self.fetch_page(url, retry_count + 1)\n",
    "                \n",
    "        except requests.HTTPError as e:\n",
    "            logger.error(f\"Erreur HTTP {e.response.status_code} pour {url}\")\n",
    "            \n",
    "        except requests.RequestException as e:\n",
    "            logger.error(f\"√âchec du chargement {url}: {e}\")\n",
    "            if retry_count < self.config.MAX_RETRIES:\n",
    "                time.sleep(self.config.DELAY_BETWEEN_REQUESTS * 2)\n",
    "                return self.fetch_page(url, retry_count + 1)\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def is_valid_url(self, url: str) -> bool:\n",
    "        \"\"\"V√©rifie si l'URL est valide et appartient au domaine cible\"\"\"\n",
    "        try:\n",
    "            parsed = urlparse(url)\n",
    "            base_parsed = urlparse(self.config.BASE_URL)\n",
    "            return parsed.netloc == base_parsed.netloc or parsed.netloc == ''\n",
    "        except:\n",
    "            return False\n",
    "    \n",
    "    def extract_links(self, soup: BeautifulSoup, base_url: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Extrait les liens pertinents d'une page\n",
    "        \"\"\"\n",
    "        links = []\n",
    "        keywords = [\n",
    "            '/publication/', '/statistique/', '/data/', '/indicator/',\n",
    "            'rapport', 'enquete', 'recensement', 'annuaire'\n",
    "        ]\n",
    "        \n",
    "        for a in soup.select(\"a[href]\"):\n",
    "            href = a.get('href', '')\n",
    "            text = a.get_text(strip=True).lower()\n",
    "            \n",
    "            # V√©rifier si le lien est pertinent\n",
    "            is_relevant = any(kw in href.lower() or kw in text for kw in keywords)\n",
    "            \n",
    "            if is_relevant and href:\n",
    "                full_url = urljoin(base_url, href)\n",
    "                if self.is_valid_url(full_url) and full_url not in self.visited_urls:\n",
    "                    links.append(full_url)\n",
    "        \n",
    "        return list(set(links))  # √âliminer les doublons\n",
    "    \n",
    "    def parse_table_data(self, table) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Parse un tableau HTML et extrait les donn√©es\n",
    "        \"\"\"\n",
    "        data = []\n",
    "        rows = table.select(\"tr\")\n",
    "        \n",
    "        # Essayer de d√©tecter les en-t√™tes\n",
    "        headers = []\n",
    "        first_row = rows[0] if rows else None\n",
    "        \n",
    "        if first_row:\n",
    "            header_cells = first_row.find_all(['th', 'td'])\n",
    "            headers = [cell.get_text(strip=True) for cell in header_cells]\n",
    "        \n",
    "        # Parser les donn√©es\n",
    "        for row in rows[1:] if headers else rows:\n",
    "            cells = row.find_all(['td', 'th'])\n",
    "            if len(cells) >= 2:\n",
    "                if headers and len(cells) == len(headers):\n",
    "                    # Utiliser les en-t√™tes comme cl√©s\n",
    "                    row_data = {headers[i]: cells[i].get_text(strip=True) \n",
    "                               for i in range(len(cells))}\n",
    "                    data.append(row_data)\n",
    "                else:\n",
    "                    # Format cl√©-valeur simple\n",
    "                    label = cells[0].get_text(strip=True)\n",
    "                    value = cells[1].get_text(strip=True)\n",
    "                    if label and value:\n",
    "                        data.append({\"indicateur\": label, \"valeur\": value})\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    def parse_indicators(self, soup: BeautifulSoup) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Extrait les indicateurs d'une page\n",
    "        \"\"\"\n",
    "        data = []\n",
    "        \n",
    "        # Chercher dans les tableaux\n",
    "        for table in soup.select(\"table\"):\n",
    "            table_data = self.parse_table_data(table)\n",
    "            data.extend(table_data)\n",
    "        \n",
    "        # Chercher dans les divs avec classes sp√©cifiques\n",
    "        for bloc in soup.select(\".indicator, .data-block, .statistics, .metric\"):\n",
    "            title = bloc.select_one(\".title, h3, h4, strong\")\n",
    "            value = bloc.select_one(\".value, .number, .data\")\n",
    "            \n",
    "            if title and value:\n",
    "                data.append({\n",
    "                    \"indicateur\": title.get_text(strip=True),\n",
    "                    \"valeur\": value.get_text(strip=True)\n",
    "                })\n",
    "        \n",
    "        # Chercher des listes de d√©finitions\n",
    "        for dl in soup.select(\"dl\"):\n",
    "            terms = dl.select(\"dt\")\n",
    "            descriptions = dl.select(\"dd\")\n",
    "            \n",
    "            for term, desc in zip(terms, descriptions):\n",
    "                data.append({\n",
    "                    \"indicateur\": term.get_text(strip=True),\n",
    "                    \"valeur\": desc.get_text(strip=True)\n",
    "                })\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    def process_page(self, url: str) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Traite une page compl√®te et retourne les donn√©es extraites\n",
    "        \"\"\"\n",
    "        html = self.fetch_page(url)\n",
    "        if not html:\n",
    "            return []\n",
    "        \n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        indicators = self.parse_indicators(soup)\n",
    "        \n",
    "        # Ajouter les m√©tadonn√©es\n",
    "        for record in indicators:\n",
    "            record['source_url'] = url\n",
    "            record['date_extraction'] = datetime.now().isoformat()\n",
    "        \n",
    "        logger.info(f\"Extrait {len(indicators)} indicateurs de {url}\")\n",
    "        return indicators\n",
    "    \n",
    "    def scrape(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Lance le processus de scraping complet\n",
    "        \"\"\"\n",
    "        logger.info(\"D√©marrage du scraping...\")\n",
    "        \n",
    "        for path in self.config.ENTRY_POINTS:\n",
    "            url = urljoin(self.config.BASE_URL, path)\n",
    "            \n",
    "            # Traiter la page principale\n",
    "            records = self.process_page(url)\n",
    "            self.all_records.extend(records)\n",
    "            \n",
    "            # R√©cup√©rer et traiter les sous-pages\n",
    "            html = self.fetch_page(url)\n",
    "            if html:\n",
    "                soup = BeautifulSoup(html, \"html.parser\")\n",
    "                sub_links = self.extract_links(soup, url)\n",
    "                \n",
    "                logger.info(f\"Trouv√© {len(sub_links)} sous-pages √† explorer\")\n",
    "                \n",
    "                for link in sub_links[:50]:  # Limiter pour √©viter trop de requ√™tes\n",
    "                    time.sleep(self.config.DELAY_BETWEEN_REQUESTS)\n",
    "                    sub_records = self.process_page(link)\n",
    "                    self.all_records.extend(sub_records)\n",
    "            \n",
    "            time.sleep(self.config.DELAY_BETWEEN_REQUESTS)\n",
    "        \n",
    "        logger.info(f\"Scraping termin√©. Total: {len(self.all_records)} enregistrements\")\n",
    "        return self.create_dataframe()\n",
    "    \n",
    "    def create_dataframe(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Cr√©e un DataFrame √† partir des donn√©es collect√©es\n",
    "        \"\"\"\n",
    "        if not self.all_records:\n",
    "            logger.warning(\"Aucune donn√©e collect√©e\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        df = pd.DataFrame(self.all_records)\n",
    "        \n",
    "        # Nettoyage des donn√©es\n",
    "        if 'valeur' in df.columns:\n",
    "            # Supprimer les espaces multiples\n",
    "            df['valeur'] = df['valeur'].str.replace(r'\\s+', ' ', regex=True)\n",
    "        \n",
    "        # Supprimer les doublons\n",
    "        df = df.drop_duplicates()\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def export_data(self, df: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        Exporte les donn√©es dans plusieurs formats\n",
    "        \"\"\"\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        \n",
    "        if df.empty:\n",
    "            logger.warning(\"Aucune donn√©e √† exporter\")\n",
    "            return\n",
    "        \n",
    "        # Export CSV\n",
    "        csv_path = self.config.OUTPUT_DIR / f\"insd_data_{timestamp}.csv\"\n",
    "        df.to_csv(csv_path, index=False, encoding='utf-8-sig')\n",
    "        logger.info(f\"Export CSV: {csv_path}\")\n",
    "        \n",
    "        # Export JSON\n",
    "        json_path = self.config.OUTPUT_DIR / f\"insd_data_{timestamp}.json\"\n",
    "        df.to_json(json_path, orient='records', force_ascii=False, indent=2)\n",
    "        logger.info(f\"Export JSON: {json_path}\")\n",
    "        \n",
    "        # Export Excel\n",
    "        try:\n",
    "            excel_path = self.config.OUTPUT_DIR / f\"insd_data_{timestamp}.xlsx\"\n",
    "            df.to_excel(excel_path, index=False, engine='openpyxl')\n",
    "            logger.info(f\"Export Excel: {excel_path}\")\n",
    "        except ImportError:\n",
    "            logger.warning(\"openpyxl non install√©, export Excel ignor√©\")\n",
    "        \n",
    "        # Statistiques\n",
    "        stats = {\n",
    "            \"total_records\": len(df),\n",
    "            \"columns\": list(df.columns),\n",
    "            \"sources_uniques\": df['source_url'].nunique() if 'source_url' in df.columns else 0,\n",
    "            \"date_extraction\": timestamp\n",
    "        }\n",
    "        \n",
    "        stats_path = self.config.OUTPUT_DIR / f\"stats_{timestamp}.json\"\n",
    "        with open(stats_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(stats, f, indent=2, ensure_ascii=False)\n",
    "        logger.info(f\"Statistiques: {stats_path}\")\n",
    "        \n",
    "        return csv_path\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Point d'entr√©e principal\"\"\"\n",
    "    try:\n",
    "        scraper = DataScraper()\n",
    "        df = scraper.scrape()\n",
    "        \n",
    "        if not df.empty:\n",
    "            scraper.export_data(df)\n",
    "            print(f\"\\n‚úÖ Scraping termin√© avec succ√®s!\")\n",
    "            print(f\"üìä {len(df)} enregistrements collect√©s\")\n",
    "            print(f\"üìÅ Fichiers sauvegard√©s dans: {scraper.config.OUTPUT_DIR}\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è Aucune donn√©e extraite\")\n",
    "            \n",
    "    except KeyboardInterrupt:\n",
    "        logger.info(\"Interruption par l'utilisateur\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Erreur fatale: {e}\", exc_info=True)\n",
    "        raise\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a6f630d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-01 10:35:13,849 - WARNING - tabula-py non install√©. Pour l'extraction de tableaux: pip install tabula-py\n",
      "2025-11-01 10:35:13,861 - INFO - D√©marrage du scraping...\n",
      "2025-11-01 10:35:13,862 - INFO - Chargement de: https://www.insd.bf/sites/default/files/2024-08/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç V√©rification des d√©pendances PDF:\n",
      "  - PyPDF2: ‚úÖ Install√©\n",
      "  - pdfplumber: ‚úÖ Install√©\n",
      "  - tabula-py: ‚ùå Non install√©\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-01 10:35:14,266 - ERROR - Erreur HTTP 404 pour https://www.insd.bf/sites/default/files/2024-08/\n",
      "2025-11-01 10:35:14,268 - INFO - Chargement de: https://www.insd.bf/sites/default/files/2024-08/\n",
      "2025-11-01 10:35:14,415 - ERROR - Erreur HTTP 404 pour https://www.insd.bf/sites/default/files/2024-08/\n",
      "2025-11-01 10:35:15,917 - INFO - Aucun PDF √† traiter\n",
      "2025-11-01 10:35:15,919 - INFO - Scraping termin√©. Total: 0 enregistrements\n",
      "2025-11-01 10:35:15,920 - INFO - PDFs trait√©s: 0\n",
      "2025-11-01 10:35:15,922 - WARNING - Aucune donn√©e collect√©e\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è  Aucune donn√©e extraite\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Script am√©lior√© de collecte de donn√©es - INSD Burkina Faso\n",
    "Collecte des indicateurs statistiques depuis burkinafaso.opendataforafrica.org\n",
    "Inclut l'extraction de donn√©es depuis les PDFs\n",
    "\"\"\"\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import logging\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from typing import List, Dict, Set, Optional, Tuple\n",
    "from datetime import datetime\n",
    "import json\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "# Imports pour le traitement des PDFs\n",
    "try:\n",
    "    import PyPDF2\n",
    "    PDF_SUPPORT = True\n",
    "except ImportError:\n",
    "    PDF_SUPPORT = False\n",
    "    logging.warning(\"PyPDF2 non install√©. Pour activer l'extraction PDF: pip install PyPDF2\")\n",
    "\n",
    "try:\n",
    "    import pdfplumber\n",
    "    PDFPLUMBER_SUPPORT = True\n",
    "except ImportError:\n",
    "    PDFPLUMBER_SUPPORT = False\n",
    "    logging.warning(\"pdfplumber non install√©. Pour une meilleure extraction: pip install pdfplumber\")\n",
    "\n",
    "try:\n",
    "    import tabula\n",
    "    TABULA_SUPPORT = True\n",
    "except ImportError:\n",
    "    TABULA_SUPPORT = False\n",
    "    logging.warning(\"tabula-py non install√©. Pour l'extraction de tableaux: pip install tabula-py\")\n",
    "\n",
    "# Configuration du logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(f'scraping_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Configuration\n",
    "class Config:\n",
    "    BASE_URL = \"https://www.insd.bf/sites/default/files/2024-08/\"\n",
    "    ENTRY_POINTS = [\n",
    "        \"\",\n",
    "        \n",
    "    ]\n",
    "    HEADERS = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (compatible; DataScraper/1.0; +https://example.org/bot)\",\n",
    "        \"Accept-Language\": \"fr-FR,fr;q=0.9\",\n",
    "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
    "    }\n",
    "    TIMEOUT = 20\n",
    "    DELAY_BETWEEN_REQUESTS = 1.5\n",
    "    MAX_RETRIES = 3\n",
    "    OUTPUT_DIR = Path(\"output\")\n",
    "    PDF_DIR = Path(\"output/pdfs\")\n",
    "    EXTRACT_PDFS = True\n",
    "    MAX_PDFS = 20  # Limite de PDFs √† t√©l√©charger\n",
    "    \n",
    "\n",
    "class PDFExtractor:\n",
    "    \"\"\"Classe pour l'extraction de donn√©es depuis les PDFs\"\"\"\n",
    "    \n",
    "    def __init__(self, pdf_path: Path):\n",
    "        self.pdf_path = pdf_path\n",
    "        self.text_content = \"\"\n",
    "        self.tables = []\n",
    "        self.metadata = {}\n",
    "    \n",
    "    def extract_with_pypdf2(self) -> str:\n",
    "        \"\"\"Extraction de texte basique avec PyPDF2\"\"\"\n",
    "        if not PDF_SUPPORT:\n",
    "            return \"\"\n",
    "        \n",
    "        try:\n",
    "            text = \"\"\n",
    "            with open(self.pdf_path, 'rb') as file:\n",
    "                reader = PyPDF2.PdfReader(file)\n",
    "                self.metadata = {\n",
    "                    'pages': len(reader.pages),\n",
    "                    'metadata': reader.metadata\n",
    "                }\n",
    "                \n",
    "                for page in reader.pages:\n",
    "                    text += page.extract_text() + \"\\n\"\n",
    "            \n",
    "            return text\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur PyPDF2 pour {self.pdf_path}: {e}\")\n",
    "            return \"\"\n",
    "    \n",
    "    def extract_with_pdfplumber(self) -> Tuple[str, List[pd.DataFrame]]:\n",
    "        \"\"\"Extraction avanc√©e avec pdfplumber (texte + tableaux)\"\"\"\n",
    "        if not PDFPLUMBER_SUPPORT:\n",
    "            return \"\", []\n",
    "        \n",
    "        try:\n",
    "            text = \"\"\n",
    "            tables = []\n",
    "            \n",
    "            with pdfplumber.open(self.pdf_path) as pdf:\n",
    "                self.metadata['pages'] = len(pdf.pages)\n",
    "                \n",
    "                for page in pdf.pages:\n",
    "                    # Extraire le texte\n",
    "                    page_text = page.extract_text()\n",
    "                    if page_text:\n",
    "                        text += page_text + \"\\n\"\n",
    "                    \n",
    "                    # Extraire les tableaux\n",
    "                    page_tables = page.extract_tables()\n",
    "                    for table in page_tables:\n",
    "                        if table:\n",
    "                            df = pd.DataFrame(table[1:], columns=table[0])\n",
    "                            tables.append(df)\n",
    "            \n",
    "            return text, tables\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur pdfplumber pour {self.pdf_path}: {e}\")\n",
    "            return \"\", []\n",
    "    \n",
    "    def extract_with_tabula(self) -> List[pd.DataFrame]:\n",
    "        \"\"\"Extraction de tableaux avec tabula-py\"\"\"\n",
    "        if not TABULA_SUPPORT:\n",
    "            return []\n",
    "        \n",
    "        try:\n",
    "            tables = tabula.read_pdf(\n",
    "                str(self.pdf_path),\n",
    "                pages='all',\n",
    "                multiple_tables=True,\n",
    "                pandas_options={'header': 'infer'}\n",
    "            )\n",
    "            return tables\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur tabula pour {self.pdf_path}: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def extract_indicators_from_text(self, text: str) -> List[Dict]:\n",
    "        \"\"\"Extrait des indicateurs statistiques du texte\"\"\"\n",
    "        indicators = []\n",
    "        \n",
    "        # Patterns pour d√©tecter les indicateurs\n",
    "        patterns = [\n",
    "            # Format: \"Indicateur : valeur\"\n",
    "            r'([A-Z√Ä-≈∏][^:]{10,100})\\s*:\\s*([0-9.,\\s%]+(?:\\s*[A-Za-z√Ä-√ø]+)?)',\n",
    "            # Format: \"- Indicateur: valeur\"\n",
    "            r'-\\s*([A-Z√Ä-≈∏][^:]{10,80})\\s*:\\s*([0-9.,\\s%]+)',\n",
    "            # Format: tableaux simples \"Description | Valeur\"\n",
    "            r'([A-Z√Ä-≈∏][^\\|]{10,80})\\s*\\|\\s*([0-9.,\\s%]+)',\n",
    "        ]\n",
    "        \n",
    "        for pattern in patterns:\n",
    "            matches = re.finditer(pattern, text, re.MULTILINE | re.IGNORECASE)\n",
    "            for match in matches:\n",
    "                label = match.group(1).strip()\n",
    "                value = match.group(2).strip()\n",
    "                \n",
    "                # Filtrer les r√©sultats non pertinents\n",
    "                if len(label) > 15 and len(value) > 0:\n",
    "                    indicators.append({\n",
    "                        'indicateur': label,\n",
    "                        'valeur': value,\n",
    "                        'type': 'text_extraction'\n",
    "                    })\n",
    "        \n",
    "        return indicators\n",
    "    \n",
    "    def extract_all(self) -> Dict:\n",
    "        \"\"\"Extraction compl√®te du PDF avec toutes les m√©thodes disponibles\"\"\"\n",
    "        logger.info(f\"Extraction du PDF: {self.pdf_path.name}\")\n",
    "        \n",
    "        result = {\n",
    "            'filename': self.pdf_path.name,\n",
    "            'text': \"\",\n",
    "            'tables': [],\n",
    "            'indicators': [],\n",
    "            'metadata': {}\n",
    "        }\n",
    "        \n",
    "        # Essayer pdfplumber en premier (le plus complet)\n",
    "        if PDFPLUMBER_SUPPORT:\n",
    "            text, tables = self.extract_with_pdfplumber()\n",
    "            result['text'] = text\n",
    "            result['tables'] = tables\n",
    "            logger.info(f\"pdfplumber: {len(text)} chars, {len(tables)} tableaux\")\n",
    "        \n",
    "        # Si pdfplumber n'est pas disponible, utiliser PyPDF2\n",
    "        elif PDF_SUPPORT:\n",
    "            text = self.extract_with_pypdf2()\n",
    "            result['text'] = text\n",
    "            logger.info(f\"PyPDF2: {len(text)} chars\")\n",
    "        \n",
    "        # Essayer tabula pour les tableaux (compl√©ment)\n",
    "        if TABULA_SUPPORT and not result['tables']:\n",
    "            tabula_tables = self.extract_with_tabula()\n",
    "            result['tables'].extend(tabula_tables)\n",
    "            logger.info(f\"tabula: {len(tabula_tables)} tableaux suppl√©mentaires\")\n",
    "        \n",
    "        # Extraire les indicateurs du texte\n",
    "        if result['text']:\n",
    "            indicators = self.extract_indicators_from_text(result['text'])\n",
    "            result['indicators'] = indicators\n",
    "            logger.info(f\"Indicateurs extraits: {len(indicators)}\")\n",
    "        \n",
    "        result['metadata'] = self.metadata\n",
    "        \n",
    "        return result\n",
    "\n",
    "\n",
    "class DataScraper:\n",
    "    \"\"\"Classe principale pour le scraping de donn√©es INSD\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Config = Config()):\n",
    "        self.config = config\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update(config.HEADERS)\n",
    "        self.visited_urls: Set[str] = set()\n",
    "        self.all_records: List[Dict] = []\n",
    "        self.pdf_data: List[Dict] = []\n",
    "        self.downloaded_pdfs: List[Path] = []\n",
    "        \n",
    "        # Cr√©er les dossiers de sortie\n",
    "        self.config.OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "        self.config.PDF_DIR.mkdir(exist_ok=True)\n",
    "    \n",
    "    def fetch_page(self, url: str, retry_count: int = 0) -> Optional[str]:\n",
    "        \"\"\"R√©cup√®re le contenu HTML d'une page avec gestion des erreurs et retry\"\"\"\n",
    "        if url in self.visited_urls:\n",
    "            logger.debug(f\"URL d√©j√† visit√©e: {url}\")\n",
    "            return None\n",
    "            \n",
    "        try:\n",
    "            logger.info(f\"Chargement de: {url}\")\n",
    "            resp = self.session.get(url, timeout=self.config.TIMEOUT)\n",
    "            resp.raise_for_status()\n",
    "            self.visited_urls.add(url)\n",
    "            return resp.text\n",
    "            \n",
    "        except requests.Timeout:\n",
    "            logger.warning(f\"Timeout pour {url}\")\n",
    "            if retry_count < self.config.MAX_RETRIES:\n",
    "                time.sleep(self.config.DELAY_BETWEEN_REQUESTS * 2)\n",
    "                return self.fetch_page(url, retry_count + 1)\n",
    "                \n",
    "        except requests.HTTPError as e:\n",
    "            logger.error(f\"Erreur HTTP {e.response.status_code} pour {url}\")\n",
    "            \n",
    "        except requests.RequestException as e:\n",
    "            logger.error(f\"√âchec du chargement {url}: {e}\")\n",
    "            if retry_count < self.config.MAX_RETRIES:\n",
    "                time.sleep(self.config.DELAY_BETWEEN_REQUESTS * 2)\n",
    "                return self.fetch_page(url, retry_count + 1)\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def download_pdf(self, url: str, filename: Optional[str] = None) -> Optional[Path]:\n",
    "        \"\"\"T√©l√©charge un PDF\"\"\"\n",
    "        try:\n",
    "            logger.info(f\"T√©l√©chargement PDF: {url}\")\n",
    "            resp = self.session.get(url, timeout=self.config.TIMEOUT * 2)\n",
    "            resp.raise_for_status()\n",
    "            \n",
    "            # D√©terminer le nom du fichier\n",
    "            if not filename:\n",
    "                filename = url.split('/')[-1]\n",
    "                if not filename.endswith('.pdf'):\n",
    "                    filename += '.pdf'\n",
    "            \n",
    "            # Nettoyer le nom de fichier\n",
    "            filename = re.sub(r'[^\\w\\-_\\. ]', '_', filename)\n",
    "            filepath = self.config.PDF_DIR / filename\n",
    "            \n",
    "            # Sauvegarder le PDF\n",
    "            with open(filepath, 'wb') as f:\n",
    "                f.write(resp.content)\n",
    "            \n",
    "            logger.info(f\"PDF t√©l√©charg√©: {filepath}\")\n",
    "            return filepath\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"√âchec du t√©l√©chargement PDF {url}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def is_valid_url(self, url: str) -> bool:\n",
    "        \"\"\"V√©rifie si l'URL est valide et appartient au domaine cible\"\"\"\n",
    "        try:\n",
    "            parsed = urlparse(url)\n",
    "            base_parsed = urlparse(self.config.BASE_URL)\n",
    "            return parsed.netloc == base_parsed.netloc or parsed.netloc == ''\n",
    "        except:\n",
    "            return False\n",
    "    \n",
    "    def extract_pdf_links(self, soup: BeautifulSoup, base_url: str) -> List[Tuple[str, str]]:\n",
    "        \"\"\"Extrait les liens vers des PDFs avec leur titre\"\"\"\n",
    "        pdf_links = []\n",
    "        \n",
    "        for a in soup.select(\"a[href]\"):\n",
    "            href = a.get('href', '')\n",
    "            text = a.get_text(strip=True)\n",
    "            \n",
    "            # V√©rifier si c'est un PDF\n",
    "            is_pdf = (\n",
    "                href.lower().endswith('.pdf') or\n",
    "                'pdf' in href.lower() or\n",
    "                'document' in href.lower() or\n",
    "                'rapport' in text.lower() or\n",
    "                'publication' in text.lower()\n",
    "            )\n",
    "            \n",
    "            if is_pdf:\n",
    "                full_url = urljoin(base_url, href)\n",
    "                if self.is_valid_url(full_url) or full_url.endswith('.pdf'):\n",
    "                    pdf_links.append((full_url, text))\n",
    "        \n",
    "        return pdf_links\n",
    "    \n",
    "    def extract_links(self, soup: BeautifulSoup, base_url: str) -> List[str]:\n",
    "        \"\"\"Extrait les liens pertinents d'une page\"\"\"\n",
    "        links = []\n",
    "        keywords = [\n",
    "            '/publication/', '/statistique/', '/data/', '/indicator/',\n",
    "            'rapport', 'enquete', 'recensement', 'annuaire'\n",
    "        ]\n",
    "        \n",
    "        for a in soup.select(\"a[href]\"):\n",
    "            href = a.get('href', '')\n",
    "            text = a.get_text(strip=True).lower()\n",
    "            \n",
    "            # V√©rifier si le lien est pertinent\n",
    "            is_relevant = any(kw in href.lower() or kw in text for kw in keywords)\n",
    "            \n",
    "            if is_relevant and href and not href.lower().endswith('.pdf'):\n",
    "                full_url = urljoin(base_url, href)\n",
    "                if self.is_valid_url(full_url) and full_url not in self.visited_urls:\n",
    "                    links.append(full_url)\n",
    "        \n",
    "        return list(set(links))  # √âliminer les doublons\n",
    "    \n",
    "    def parse_table_data(self, table) -> List[Dict]:\n",
    "        \"\"\"Parse un tableau HTML et extrait les donn√©es\"\"\"\n",
    "        data = []\n",
    "        rows = table.select(\"tr\")\n",
    "        \n",
    "        # Essayer de d√©tecter les en-t√™tes\n",
    "        headers = []\n",
    "        first_row = rows[0] if rows else None\n",
    "        \n",
    "        if first_row:\n",
    "            header_cells = first_row.find_all(['th', 'td'])\n",
    "            headers = [cell.get_text(strip=True) for cell in header_cells]\n",
    "        \n",
    "        # Parser les donn√©es\n",
    "        for row in rows[1:] if headers else rows:\n",
    "            cells = row.find_all(['td', 'th'])\n",
    "            if len(cells) >= 2:\n",
    "                if headers and len(cells) == len(headers):\n",
    "                    row_data = {headers[i]: cells[i].get_text(strip=True) \n",
    "                               for i in range(len(cells))}\n",
    "                    data.append(row_data)\n",
    "                else:\n",
    "                    label = cells[0].get_text(strip=True)\n",
    "                    value = cells[1].get_text(strip=True)\n",
    "                    if label and value:\n",
    "                        data.append({\"indicateur\": label, \"valeur\": value})\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    def parse_indicators(self, soup: BeautifulSoup) -> List[Dict]:\n",
    "        \"\"\"Extrait les indicateurs d'une page\"\"\"\n",
    "        data = []\n",
    "        \n",
    "        # Chercher dans les tableaux\n",
    "        for table in soup.select(\"table\"):\n",
    "            table_data = self.parse_table_data(table)\n",
    "            data.extend(table_data)\n",
    "        \n",
    "        # Chercher dans les divs avec classes sp√©cifiques\n",
    "        for bloc in soup.select(\".indicator, .data-block, .statistics, .metric\"):\n",
    "            title = bloc.select_one(\".title, h3, h4, strong\")\n",
    "            value = bloc.select_one(\".value, .number, .data\")\n",
    "            \n",
    "            if title and value:\n",
    "                data.append({\n",
    "                    \"indicateur\": title.get_text(strip=True),\n",
    "                    \"valeur\": value.get_text(strip=True)\n",
    "                })\n",
    "        \n",
    "        # Chercher des listes de d√©finitions\n",
    "        for dl in soup.select(\"dl\"):\n",
    "            terms = dl.select(\"dt\")\n",
    "            descriptions = dl.select(\"dd\")\n",
    "            \n",
    "            for term, desc in zip(terms, descriptions):\n",
    "                data.append({\n",
    "                    \"indicateur\": term.get_text(strip=True),\n",
    "                    \"valeur\": desc.get_text(strip=True)\n",
    "                })\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    def process_page(self, url: str) -> List[Dict]:\n",
    "        \"\"\"Traite une page compl√®te et retourne les donn√©es extraites\"\"\"\n",
    "        html = self.fetch_page(url)\n",
    "        if not html:\n",
    "            return []\n",
    "        \n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        indicators = self.parse_indicators(soup)\n",
    "        \n",
    "        # Ajouter les m√©tadonn√©es\n",
    "        for record in indicators:\n",
    "            record['source_url'] = url\n",
    "            record['source_type'] = 'html'\n",
    "            record['date_extraction'] = datetime.now().isoformat()\n",
    "        \n",
    "        logger.info(f\"Extrait {len(indicators)} indicateurs de {url}\")\n",
    "        return indicators\n",
    "    \n",
    "    def process_pdfs(self):\n",
    "        \"\"\"Traite tous les PDFs t√©l√©charg√©s\"\"\"\n",
    "        if not self.downloaded_pdfs:\n",
    "            logger.info(\"Aucun PDF √† traiter\")\n",
    "            return\n",
    "        \n",
    "        logger.info(f\"Traitement de {len(self.downloaded_pdfs)} PDFs...\")\n",
    "        \n",
    "        for pdf_path in self.downloaded_pdfs:\n",
    "            try:\n",
    "                extractor = PDFExtractor(pdf_path)\n",
    "                pdf_data = extractor.extract_all()\n",
    "                \n",
    "                # Ajouter les indicateurs extraits\n",
    "                for indicator in pdf_data['indicators']:\n",
    "                    indicator['source_file'] = pdf_path.name\n",
    "                    indicator['source_type'] = 'pdf'\n",
    "                    indicator['date_extraction'] = datetime.now().isoformat()\n",
    "                \n",
    "                self.all_records.extend(pdf_data['indicators'])\n",
    "                \n",
    "                # Ajouter les tableaux\n",
    "                for i, table in enumerate(pdf_data['tables']):\n",
    "                    if not table.empty:\n",
    "                        for _, row in table.iterrows():\n",
    "                            record = row.to_dict()\n",
    "                            record['source_file'] = pdf_path.name\n",
    "                            record['source_type'] = 'pdf_table'\n",
    "                            record['table_index'] = i\n",
    "                            record['date_extraction'] = datetime.now().isoformat()\n",
    "                            self.all_records.append(record)\n",
    "                \n",
    "                self.pdf_data.append(pdf_data)\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Erreur lors du traitement de {pdf_path}: {e}\")\n",
    "    \n",
    "    def scrape(self) -> pd.DataFrame:\n",
    "        \"\"\"Lance le processus de scraping complet\"\"\"\n",
    "        logger.info(\"D√©marrage du scraping...\")\n",
    "        pdf_count = 0\n",
    "        \n",
    "        for path in self.config.ENTRY_POINTS:\n",
    "            url = urljoin(self.config.BASE_URL, path)\n",
    "            \n",
    "            # Traiter la page principale\n",
    "            records = self.process_page(url)\n",
    "            self.all_records.extend(records)\n",
    "            \n",
    "            # R√©cup√©rer le contenu pour extraire les liens\n",
    "            html = self.fetch_page(url)\n",
    "            if html:\n",
    "                soup = BeautifulSoup(html, \"html.parser\")\n",
    "                \n",
    "                # Extraire et t√©l√©charger les PDFs\n",
    "                if self.config.EXTRACT_PDFS and pdf_count < self.config.MAX_PDFS:\n",
    "                    pdf_links = self.extract_pdf_links(soup, url)\n",
    "                    logger.info(f\"Trouv√© {len(pdf_links)} PDFs potentiels\")\n",
    "                    \n",
    "                    for pdf_url, pdf_title in pdf_links:\n",
    "                        if pdf_count >= self.config.MAX_PDFS:\n",
    "                            break\n",
    "                        \n",
    "                        time.sleep(self.config.DELAY_BETWEEN_REQUESTS)\n",
    "                        pdf_path = self.download_pdf(pdf_url, pdf_title)\n",
    "                        if pdf_path:\n",
    "                            self.downloaded_pdfs.append(pdf_path)\n",
    "                            pdf_count += 1\n",
    "                \n",
    "                # Extraire et traiter les sous-pages\n",
    "                sub_links = self.extract_links(soup, url)\n",
    "                logger.info(f\"Trouv√© {len(sub_links)} sous-pages √† explorer\")\n",
    "                \n",
    "                for link in sub_links[:50]:\n",
    "                    time.sleep(self.config.DELAY_BETWEEN_REQUESTS)\n",
    "                    sub_records = self.process_page(link)\n",
    "                    self.all_records.extend(sub_records)\n",
    "            \n",
    "            time.sleep(self.config.DELAY_BETWEEN_REQUESTS)\n",
    "        \n",
    "        # Traiter les PDFs t√©l√©charg√©s\n",
    "        if self.config.EXTRACT_PDFS:\n",
    "            self.process_pdfs()\n",
    "        \n",
    "        logger.info(f\"Scraping termin√©. Total: {len(self.all_records)} enregistrements\")\n",
    "        logger.info(f\"PDFs trait√©s: {len(self.downloaded_pdfs)}\")\n",
    "        \n",
    "        return self.create_dataframe()\n",
    "    \n",
    "    def create_dataframe(self) -> pd.DataFrame:\n",
    "        \"\"\"Cr√©e un DataFrame √† partir des donn√©es collect√©es\"\"\"\n",
    "        if not self.all_records:\n",
    "            logger.warning(\"Aucune donn√©e collect√©e\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        df = pd.DataFrame(self.all_records)\n",
    "        \n",
    "        # Nettoyage des donn√©es\n",
    "        for col in df.columns:\n",
    "            if df[col].dtype == 'object':\n",
    "                df[col] = df[col].astype(str).str.replace(r'\\s+', ' ', regex=True)\n",
    "                df[col] = df[col].str.strip()\n",
    "        \n",
    "        # Supprimer les doublons\n",
    "        df = df.drop_duplicates()\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def export_data(self, df: pd.DataFrame):\n",
    "        \"\"\"Exporte les donn√©es dans plusieurs formats\"\"\"\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        \n",
    "        if df.empty:\n",
    "            logger.warning(\"Aucune donn√©e √† exporter\")\n",
    "            return\n",
    "        \n",
    "        # Export CSV\n",
    "        csv_path = self.config.OUTPUT_DIR / f\"insd_data_{timestamp}.csv\"\n",
    "        df.to_csv(csv_path, index=False, encoding='utf-8-sig')\n",
    "        logger.info(f\"Export CSV: {csv_path}\")\n",
    "        \n",
    "        # Export JSON\n",
    "        json_path = self.config.OUTPUT_DIR / f\"insd_data_{timestamp}.json\"\n",
    "        df.to_json(json_path, orient='records', force_ascii=False, indent=2)\n",
    "        logger.info(f\"Export JSON: {json_path}\")\n",
    "        \n",
    "        # Export Excel\n",
    "        try:\n",
    "            excel_path = self.config.OUTPUT_DIR / f\"insd_data_{timestamp}.xlsx\"\n",
    "            \n",
    "            with pd.ExcelWriter(excel_path, engine='openpyxl') as writer:\n",
    "                # Feuille principale avec toutes les donn√©es\n",
    "                df.to_excel(writer, sheet_name='Toutes_donnees', index=False)\n",
    "                \n",
    "                # Feuilles s√©par√©es par type de source\n",
    "                if 'source_type' in df.columns:\n",
    "                    for source_type in df['source_type'].unique():\n",
    "                        df_type = df[df['source_type'] == source_type]\n",
    "                        sheet_name = f\"Source_{source_type}\"[:31]  # Limite Excel\n",
    "                        df_type.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "            \n",
    "            logger.info(f\"Export Excel: {excel_path}\")\n",
    "        except ImportError:\n",
    "            logger.warning(\"openpyxl non install√©, export Excel ignor√©\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur export Excel: {e}\")\n",
    "        \n",
    "        # Statistiques d√©taill√©es\n",
    "        stats = {\n",
    "            \"total_records\": len(df),\n",
    "            \"columns\": list(df.columns),\n",
    "            \"date_extraction\": timestamp,\n",
    "            \"pdfs_downloaded\": len(self.downloaded_pdfs),\n",
    "            \"pdf_files\": [p.name for p in self.downloaded_pdfs]\n",
    "        }\n",
    "        \n",
    "        if 'source_type' in df.columns:\n",
    "            stats['records_by_source'] = df['source_type'].value_counts().to_dict()\n",
    "        \n",
    "        if 'source_url' in df.columns:\n",
    "            stats['sources_html_uniques'] = df[df.get('source_type', '') == 'html']['source_url'].nunique()\n",
    "        \n",
    "        stats_path = self.config.OUTPUT_DIR / f\"stats_{timestamp}.json\"\n",
    "        with open(stats_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(stats, f, indent=2, ensure_ascii=False)\n",
    "        logger.info(f\"Statistiques: {stats_path}\")\n",
    "        \n",
    "        # Export des m√©tadonn√©es PDF\n",
    "        if self.pdf_data:\n",
    "            pdf_metadata = []\n",
    "            for pdf in self.pdf_data:\n",
    "                pdf_metadata.append({\n",
    "                    'filename': pdf['filename'],\n",
    "                    'text_length': len(pdf['text']),\n",
    "                    'tables_count': len(pdf['tables']),\n",
    "                    'indicators_count': len(pdf['indicators']),\n",
    "                    'metadata': pdf['metadata']\n",
    "                })\n",
    "            \n",
    "            pdf_meta_path = self.config.OUTPUT_DIR / f\"pdf_metadata_{timestamp}.json\"\n",
    "            with open(pdf_meta_path, 'w', encoding='utf-8') as f:\n",
    "                json.dump(pdf_metadata, f, indent=2, ensure_ascii=False)\n",
    "            logger.info(f\"M√©tadonn√©es PDF: {pdf_meta_path}\")\n",
    "        \n",
    "        return csv_path\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Point d'entr√©e principal\"\"\"\n",
    "    \n",
    "    # Afficher les d√©pendances disponibles\n",
    "    print(\"üîç V√©rification des d√©pendances PDF:\")\n",
    "    print(f\"  - PyPDF2: {'‚úÖ Install√©' if PDF_SUPPORT else '‚ùå Non install√©'}\")\n",
    "    print(f\"  - pdfplumber: {'‚úÖ Install√©' if PDFPLUMBER_SUPPORT else '‚ùå Non install√©'}\")\n",
    "    print(f\"  - tabula-py: {'‚úÖ Install√©' if TABULA_SUPPORT else '‚ùå Non install√©'}\")\n",
    "    print()\n",
    "    \n",
    "    if not any([PDF_SUPPORT, PDFPLUMBER_SUPPORT, TABULA_SUPPORT]):\n",
    "        print(\"‚ö†Ô∏è  Aucune biblioth√®que PDF install√©e.\")\n",
    "        print(\"üì¶ Pour installer toutes les d√©pendances:\")\n",
    "        print(\"   pip install PyPDF2 pdfplumber tabula-py openpyxl\")\n",
    "        print()\n",
    "        response = input(\"Continuer sans extraction PDF? (o/N): \")\n",
    "        if response.lower() != 'o':\n",
    "            return\n",
    "        Config.EXTRACT_PDFS = False\n",
    "    \n",
    "    try:\n",
    "        scraper = DataScraper()\n",
    "        df = scraper.scrape()\n",
    "        \n",
    "        if not df.empty:\n",
    "            scraper.export_data(df)\n",
    "            print(f\"\\n‚úÖ Scraping termin√© avec succ√®s!\")\n",
    "            print(f\"üìä {len(df)} enregistrements collect√©s\")\n",
    "            print(f\"üìÑ {len(scraper.downloaded_pdfs)} PDFs t√©l√©charg√©s et trait√©s\")\n",
    "            print(f\"üìÅ Fichiers sauvegard√©s dans: {scraper.config.OUTPUT_DIR}\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è  Aucune donn√©e extraite\")\n",
    "            \n",
    "    except KeyboardInterrupt:\n",
    "        logger.info(\"Interruption par l'utilisateur\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Erreur fatale: {e}\", exc_info=True)\n",
    "        raise\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2cf3e6c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'fastapi'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 18\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m List, Optional\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m#from pydantic import BaseModel\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfastapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FastAPI, HTTPException\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Embeddings\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'fastapi'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "main.py\n",
    "Impl√©mentation RAG minimaliste, locale et open-source :\n",
    "- Embeddings : sentence-transformers (all-MiniLM-L6-v2)\n",
    "- Vector DB : FAISS + SQLite (sqlite-utils for convenience)\n",
    "- LLM : llama-cpp-python (local ggml) OR transformers fallback\n",
    "\n",
    "Endpoints:\n",
    "- POST /ingest  -> {\"docs\": [{\"id\": \"doc1\", \"text\": \"...\", \"meta\": {...}}, ...]}\n",
    "- POST /query   -> {\"question\": \"...\" , \"k\": 5}\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import sqlite3\n",
    "from typing import List, Optional\n",
    "#from pydantic import BaseModel\n",
    "from fastapi import FastAPI, HTTPException\n",
    "import numpy as np\n",
    "\n",
    "# Embeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# FAISS\n",
    "import faiss\n",
    "\n",
    "# SQLite helper\n",
    "from sqlite_utils import Database\n",
    "\n",
    "# LLM (two options)\n",
    "try:\n",
    "    # Prefer llama-cpp-python if available (local ggml)\n",
    "    from llama_cpp import Llama\n",
    "    LLAMA_AVAILABLE = True\n",
    "except Exception:\n",
    "    LLAMA_AVAILABLE = False\n",
    "\n",
    "try:\n",
    "    from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "    TRANSFORMERS_AVAILABLE = True\n",
    "except Exception:\n",
    "    TRANSFORMERS_AVAILABLE = False\n",
    "\n",
    "# --------------------------\n",
    "# Configuration\n",
    "# --------------------------\n",
    "DATA_DIR = os.environ.get(\"RAG_DATA_DIR\", \"./rag_data\")\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "FAISS_INDEX_PATH = os.path.join(DATA_DIR, \"faiss.index\")\n",
    "SQLITE_PATH = os.path.join(DATA_DIR, \"metastore.db\")\n",
    "EMBED_MODEL_NAME = os.environ.get(\"EMBED_MODEL\", \"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "# LLM options (set via env or edit here)\n",
    "LLAMA_GGML_PATH = os.environ.get(\"LLAMA_GGML_PATH\", None)  # ex: \"./models/ggml-model-q4_0.bin\"\n",
    "FALLBACK_TRANSFORMER = os.environ.get(\"FALLBACK_MODEL\", \"tiiuae/falcon-7b-instruct\")  # user can change\n",
    "\n",
    "EMBED_DIM = 384  # all-MiniLM-L6-v2 ‚Üí 384 dims\n",
    "\n",
    "# --------------------------\n",
    "# Init components\n",
    "# --------------------------\n",
    "# Embedding model\n",
    "embedder = SentenceTransformer(EMBED_MODEL_NAME)\n",
    "\n",
    "# SQLite metadata DB via sqlite-utils for convenience\n",
    "db = Database(SQLITE_PATH)\n",
    "if \"documents\" not in db.table_names():\n",
    "    db[\"documents\"].create({\n",
    "        \"id\": str,\n",
    "        \"text\": str,\n",
    "        \"meta\": str,\n",
    "        \"embedding_id\": int\n",
    "    }, pk=\"id\")\n",
    "\n",
    "# FAISS index (IndexFlatIP for cosine-similarity with normalized vectors)\n",
    "# We'll store normalized embeddings to use inner product as cosine-similarity\n",
    "if os.path.exists(FAISS_INDEX_PATH):\n",
    "    print(\"Loading FAISS index from disk...\")\n",
    "    index = faiss.read_index(FAISS_INDEX_PATH)\n",
    "    # ensure index has correct dimension (assume embedded dimension matches)\n",
    "else:\n",
    "    index = faiss.IndexFlatIP(EMBED_DIM)\n",
    "    # optionally wrap in IndexIDMap to keep stable ids\n",
    "    index = faiss.IndexIDMap(index)\n",
    "\n",
    "# Keep track of next embedding id\n",
    "def get_next_embedding_id():\n",
    "    cur = db[\"documents\"].conn.execute(\"SELECT MAX(embedding_id) FROM documents\").fetchone()[0]\n",
    "    return (int(cur) + 1) if cur is not None else 0\n",
    "\n",
    "# --------------------------\n",
    "# LLM Setup\n",
    "# --------------------------\n",
    "llm_client = None\n",
    "use_llama = False\n",
    "if LLAMA_AVAILABLE and LLAMA_GGML_PATH and os.path.exists(LLAMA_GGML_PATH):\n",
    "    print(\"Using llama-cpp-python with model:\", LLAMA_GGML_PATH)\n",
    "    llm_client = Llama(model_path=LLAMA_GGML_PATH)\n",
    "    use_llama = True\n",
    "elif TRANSFORMERS_AVAILABLE:\n",
    "    # fallback: load a small-to-medium instruct model (user responsibility to provide one that fits RAM)\n",
    "    print(\"Using transformers fallback model:\", FALLBACK_TRANSFORMER)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(FALLBACK_TRANSFORMER)\n",
    "    model = AutoModelForCausalLM.from_pretrained(FALLBACK_TRANSFORMER, device_map=\"auto\")\n",
    "    text_gen = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device=0 if hasattr(model, \"device\") else -1)\n",
    "else:\n",
    "    print(\"No LLM backend available. Install llama-cpp-python or transformers.\")\n",
    "\n",
    "# --------------------------\n",
    "# Utilities\n",
    "# --------------------------\n",
    "def normalize_embeddings(vectors: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Normalize rows to unit length (for cosine similarity using inner product).\"\"\"\n",
    "    norms = np.linalg.norm(vectors, axis=1, keepdims=True)\n",
    "    norms[norms==0] = 1e-10\n",
    "    return vectors / norms\n",
    "\n",
    "def embed_texts(texts: List[str]) -> np.ndarray:\n",
    "    embs = embedder.encode(texts, convert_to_numpy=True, show_progress_bar=False)\n",
    "    if embs.ndim == 1:\n",
    "        embs = embs.reshape(1, -1)\n",
    "    return normalize_embeddings(embs)\n",
    "\n",
    "def add_documents(docs: List[dict]):\n",
    "    \"\"\"\n",
    "    docs: list of {\"id\": str, \"text\": str, \"meta\": dict}\n",
    "    \"\"\"\n",
    "    if not docs:\n",
    "        return\n",
    "    texts = [d[\"text\"] for d in docs]\n",
    "    embeddings = embed_texts(texts)\n",
    "    start_id = get_next_embedding_id()\n",
    "    ids = np.arange(start_id, start_id + len(docs)).astype(\"int64\")\n",
    "    # add to FAISS\n",
    "    index.add_with_ids(embeddings.astype(\"float32\"), ids)\n",
    "    # store metadata in sqlite\n",
    "    for doc, emb_id in zip(docs, ids):\n",
    "        db[\"documents\"].insert({\n",
    "            \"id\": doc[\"id\"],\n",
    "            \"text\": doc[\"text\"],\n",
    "            \"meta\": json.dumps(doc.get(\"meta\", {}), ensure_ascii=False),\n",
    "            \"embedding_id\": int(emb_id)\n",
    "        }, replace=True)\n",
    "    # persist faiss\n",
    "    faiss.write_index(index, FAISS_INDEX_PATH)\n",
    "\n",
    "def search(query: str, k: int = 5):\n",
    "    q_emb = embed_texts([query]).astype(\"float32\")\n",
    "    if index.ntotal == 0:\n",
    "        return []\n",
    "    scores, ids = index.search(q_emb, k)\n",
    "    scores = scores[0].tolist()\n",
    "    ids = ids[0].tolist()\n",
    "    results = []\n",
    "    for sid, sc in zip(ids, scores):\n",
    "        if sid == -1:\n",
    "            continue\n",
    "        r = db[\"documents\"].get(sid, where=\"embedding_id = ?\", columns=[\"id\", \"text\", \"meta\", \"embedding_id\"])\n",
    "        # Because we used embedding_id as numeric IDs, retrieve via query\n",
    "        cur = db[\"documents\"].conn.execute(\"SELECT id, text, meta, embedding_id FROM documents WHERE embedding_id = ?\", (sid,))\n",
    "        row = cur.fetchone()\n",
    "        if not row:\n",
    "            continue\n",
    "        doc_id, text, meta_json, emb_id = row\n",
    "        meta = json.loads(meta_json) if meta_json else {}\n",
    "        results.append({\n",
    "            \"id\": doc_id,\n",
    "            \"text\": text,\n",
    "            \"meta\": meta,\n",
    "            \"score\": float(sc),\n",
    "            \"embedding_id\": int(emb_id)\n",
    "        })\n",
    "    return results\n",
    "\n",
    "# --------------------------\n",
    "# Prompt assembly for RAG\n",
    "# --------------------------\n",
    "def build_rag_prompt(question: str, docs: List[dict], max_len_chars: int = 3000) -> str:\n",
    "    \"\"\"\n",
    "    Assemble prompt: include question, retrieved docs (with source), and instructions.\n",
    "    Trim docs if needed to fit max_len_chars.\n",
    "    \"\"\"\n",
    "    instruction = (\n",
    "        \"Tu es un assistant utile. Utilise les documents fournis pour r√©pondre pr√©cis√©ment √† la question.\\n\"\n",
    "        \"Cite les sources sous forme [source:id] √† la fin de la r√©ponse.\\n\\n\"\n",
    "    )\n",
    "    context_blocks = []\n",
    "    total = 0\n",
    "    for d in docs:\n",
    "        block = f\"[source:{d['id']}]\\n{d['text']}\\n\\n\"\n",
    "        if total + len(block) > max_len_chars:\n",
    "            break\n",
    "        context_blocks.append(block)\n",
    "        total += len(block)\n",
    "    context = \"\\n\".join(context_blocks)\n",
    "    prompt = instruction + \"Contexte :\\n\" + context + \"\\nQuestion : \" + question + \"\\nR√©ponse :\"\n",
    "    return prompt\n",
    "\n",
    "# --------------------------\n",
    "# LLM Generator\n",
    "# --------------------------\n",
    "def generate_answer(prompt: str, max_tokens: int = 512, temperature: float = 0.2) -> str:\n",
    "    if use_llama and llm_client:\n",
    "        # llama-cpp-python interface\n",
    "        resp = llm_client.create(prompt=prompt, max_tokens=max_tokens, temperature=temperature)\n",
    "        return resp.get(\"choices\", [{}])[0].get(\"text\", \"\").strip()\n",
    "    elif TRANSFORMERS_AVAILABLE:\n",
    "        # transformers pipeline\n",
    "        outputs = text_gen(prompt, max_length=len(prompt.split()) + max_tokens, do_sample=True, temperature=temperature, num_return_sequences=1)\n",
    "        return outputs[0][\"generated_text\"][len(prompt):].strip()\n",
    "    else:\n",
    "        raise RuntimeError(\"Aucun backend LLM disponible. Installez llama-cpp-python (et un mod√®le ggml) ou transformers.\")\n",
    "\n",
    "# --------------------------\n",
    "# FastAPI\n",
    "# --------------------------\n",
    "app = FastAPI(title=\"RAG Open-Source (FAISS + SentenceTransformers + LLM)\")\n",
    "\n",
    "class DocItem(BaseModel):\n",
    "    id: str\n",
    "    text: str\n",
    "    meta: Optional[dict] = {}\n",
    "\n",
    "class IngestRequest(BaseModel):\n",
    "    docs: List[DocItem]\n",
    "\n",
    "class QueryRequest(BaseModel):\n",
    "    question: str\n",
    "    k: Optional[int] = 5\n",
    "    max_context_chars: Optional[int] = 3000\n",
    "\n",
    "@app.post(\"/ingest\")\n",
    "def ingest(payload: IngestRequest):\n",
    "    try:\n",
    "        items = []\n",
    "        for d in payload.docs:\n",
    "            items.append({\"id\": d.id, \"text\": d.text, \"meta\": d.meta})\n",
    "        add_documents(items)\n",
    "        return {\"status\": \"ok\", \"added\": len(items)}\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "@app.post(\"/query\")\n",
    "def query(payload: QueryRequest):\n",
    "    if index.ntotal == 0:\n",
    "        raise HTTPException(400, \"Index vide ‚Äî ing√©rez d'abord des documents via /ingest.\")\n",
    "    results = search(payload.question, k=payload.k)\n",
    "    if not results:\n",
    "        return {\"answer\": \"\", \"sources\": [], \"retrieved\": []}\n",
    "    prompt = build_rag_prompt(payload.question, results, max_len_chars=payload.max_context_chars)\n",
    "    try:\n",
    "        answer = generate_answer(prompt)\n",
    "    except Exception as e:\n",
    "        raise HTTPException(500, f\"Erreur LLM: {e}\")\n",
    "    sources = [r[\"id\"] for r in results]\n",
    "    return {\"answer\": answer, \"sources\": sources, \"retrieved\": results}\n",
    "\n",
    "# health\n",
    "@app.get(\"/health\")\n",
    "def health():\n",
    "    return {\"status\": \"ok\", \"index_size\": int(index.ntotal)}\n",
    "\n",
    "# --------------------------\n",
    "# Simple CLI ingestion helper (optional)\n",
    "# --------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    import argparse\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--run\", action=\"store_true\", help=\"Run API\")\n",
    "    parser.add_argument(\"--ingest-file\", type=str, help=\"JSON file with docs list [{id,text,meta},...]\")\n",
    "    parser.add_argument(\"--host\", default=\"0.0.0.0\")\n",
    "    parser.add_argument(\"--port\", type=int, default=8000)\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    if args.ingest_file:\n",
    "        with open(args.ingest_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            docs = json.load(f)\n",
    "        add_documents(docs)\n",
    "        print(f\"Ing√©r√© {len(docs)} documents.\")\n",
    "    if args.run:\n",
    "        import uvicorn\n",
    "        uvicorn.run(\"main:app\", host=args.host, port=args.port, reload=True)\n",
    "\n",
    "\n",
    "#python -m venv venv\n",
    "#venv\\Scripts\\activate.bat\n",
    "#pip install -r requirements.txt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
