{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89ecdf51",
   "metadata": {},
   "source": [
    "# Systeme de RAG open source complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2085341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Initialisation du syst√®me RAG...\n",
      "üìä Chargement du mod√®le d'embeddings...\n",
      "üíæ Initialisation de ChromaDB...\n",
      "ü§ñ LLM: Mistral (Ollama local)\n",
      "‚úÖ Syst√®me RAG initialis√© avec succ√®s!\n",
      "\n",
      "üìù Ajout de 4 documents...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67e886d2195e4827a44f1c1ab86c8f9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ 4 documents ajout√©s avec succ√®s!\n",
      "\n",
      "============================================================\n",
      "D√âMONSTRATION DU SYST√àME RAG\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "QUESTION 1\n",
      "============================================================\n",
      "‚ùì Question: Qu'est-ce que le RAG ?\n",
      "\n",
      "üîç Recherche des documents pertinents...\n",
      "üìö 2 documents trouv√©s\n",
      "\n",
      "ü§ñ G√©n√©ration de la r√©ponse...\n",
      "\n",
      "üí° R√âPONSE:\n",
      "‚ùå Erreur lors de la g√©n√©ration: 404 Client Error: Not Found for url: http://localhost:11434/api/generate\n",
      "\n",
      "üìñ SOURCES UTILIS√âES:\n",
      "\n",
      "  [1] Similarit√©: 45.6%\n",
      "      Sujet: RAG\n",
      "      Extrait: Le RAG (Retrieval-Augmented Generation) est une technique qui combine la recherche \n",
      "        d'inform...\n",
      "\n",
      "  [2] Similarit√©: 19.5%\n",
      "      Sujet: ChromaDB\n",
      "      Extrait: ChromaDB est une base de donn√©es vectorielle open source optimis√©e pour les embeddings. \n",
      "        Ell...\n",
      "\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "QUESTION 2\n",
      "============================================================\n",
      "‚ùì Question: Pourquoi utiliser ChromaDB ?\n",
      "\n",
      "üîç Recherche des documents pertinents...\n",
      "üìö 2 documents trouv√©s\n",
      "\n",
      "ü§ñ G√©n√©ration de la r√©ponse...\n",
      "\n",
      "üí° R√âPONSE:\n",
      "‚ùå Erreur lors de la g√©n√©ration: 404 Client Error: Not Found for url: http://localhost:11434/api/generate\n",
      "\n",
      "üìñ SOURCES UTILIS√âES:\n",
      "\n",
      "  [1] Similarit√©: 63.6%\n",
      "      Sujet: ChromaDB\n",
      "      Extrait: ChromaDB est une base de donn√©es vectorielle open source optimis√©e pour les embeddings. \n",
      "        Ell...\n",
      "\n",
      "  [2] Similarit√©: 25.1%\n",
      "      Sujet: RAG\n",
      "      Extrait: Le RAG (Retrieval-Augmented Generation) est une technique qui combine la recherche \n",
      "        d'inform...\n",
      "\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "QUESTION 3\n",
      "============================================================\n",
      "‚ùì Question: Quelles sont les capacit√©s de Mistral ?\n",
      "\n",
      "üîç Recherche des documents pertinents...\n",
      "üìö 2 documents trouv√©s\n",
      "\n",
      "ü§ñ G√©n√©ration de la r√©ponse...\n",
      "\n",
      "üí° R√âPONSE:\n",
      "‚ùå Erreur lors de la g√©n√©ration: 404 Client Error: Not Found for url: http://localhost:11434/api/generate\n",
      "\n",
      "üìñ SOURCES UTILIS√âES:\n",
      "\n",
      "  [1] Similarit√©: 60.8%\n",
      "      Sujet: Mistral\n",
      "      Extrait: Mistral est un mod√®le de langage open source d√©velopp√© par Mistral AI en France. \n",
      "        Il offre d...\n",
      "\n",
      "  [2] Similarit√©: 16.8%\n",
      "      Sujet: IA\n",
      "      Extrait: L'intelligence artificielle (IA) est l'ensemble des th√©ories et des techniques \n",
      "        mises en ≈ìuv...\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Syst√®me RAG 100% Open Source\n",
    "Embeddings: sentence-transformers\n",
    "VectorDB: ChromaDB\n",
    "LLM: Mistral via Ollama (ou Groq)\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from typing import List, Dict\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import requests\n",
    "import json\n",
    "\n",
    "class RAGSystem:\n",
    "    def __init__(self, use_local_llm=True):\n",
    "        \"\"\"\n",
    "        Initialise le syst√®me RAG\n",
    "        \n",
    "        Args:\n",
    "            use_local_llm: True pour Ollama local, False pour API Groq\n",
    "        \"\"\"\n",
    "        print(\"üöÄ Initialisation du syst√®me RAG...\")\n",
    "        \n",
    "        # 1. Mod√®le d'embeddings (multilingue, l√©ger)\n",
    "        print(\"üìä Chargement du mod√®le d'embeddings...\")\n",
    "        self.embedding_model = SentenceTransformer(\n",
    "            'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'\n",
    "        )\n",
    "        \n",
    "        # 2. Base de donn√©es vectorielle ChromaDB\n",
    "        print(\"üíæ Initialisation de ChromaDB...\")\n",
    "        self.chroma_client = chromadb.Client(Settings(\n",
    "            anonymized_telemetry=False,\n",
    "            allow_reset=True\n",
    "        ))\n",
    "        \n",
    "        # Cr√©er ou r√©cup√©rer une collection\n",
    "        self.collection = self.chroma_client.get_or_create_collection(\n",
    "            name=\"documents\",\n",
    "            metadata={\"hnsw:space\": \"cosine\"}  # Similarit√© cosinus\n",
    "        )\n",
    "        \n",
    "        # 3. Configuration du LLM\n",
    "        self.use_local_llm = use_local_llm\n",
    "        if use_local_llm:\n",
    "            self.llm_url = \"http://localhost:11434/api/generate\"\n",
    "            self.llm_model = \"mistral\"\n",
    "            print(\"ü§ñ LLM: Mistral (Ollama local)\")\n",
    "        else:\n",
    "            # Pour utiliser Groq, d√©finir GROQ_API_KEY dans .env\n",
    "            self.groq_api_key = os.getenv(\"GROQ_API_KEY\", \"\")\n",
    "            self.llm_model = \"llama3-8b-8192\"\n",
    "            print(\"ü§ñ LLM: Groq API\")\n",
    "        \n",
    "        print(\"‚úÖ Syst√®me RAG initialis√© avec succ√®s!\\n\")\n",
    "    \n",
    "    def add_documents(self, documents: List[str], metadatas: List[Dict] = None):\n",
    "        \"\"\"\n",
    "        Ajoute des documents √† la base vectorielle\n",
    "        \n",
    "        Args:\n",
    "            documents: Liste de textes √† indexer\n",
    "            metadatas: M√©tadonn√©es optionnelles pour chaque document\n",
    "        \"\"\"\n",
    "        print(f\"üìù Ajout de {len(documents)} documents...\")\n",
    "        \n",
    "        # G√©n√©rer les embeddings\n",
    "        embeddings = self.embedding_model.encode(\n",
    "            documents,\n",
    "            show_progress_bar=True,\n",
    "            convert_to_numpy=True\n",
    "        )\n",
    "        \n",
    "        # Cr√©er des IDs uniques\n",
    "        ids = [f\"doc_{i}\" for i in range(len(documents))]\n",
    "        \n",
    "        # Pr√©parer les m√©tadonn√©es si non fournies\n",
    "        if metadatas is None:\n",
    "            metadatas = [{\"source\": f\"document_{i}\"} for i in range(len(documents))]\n",
    "        \n",
    "        # Ajouter √† ChromaDB\n",
    "        self.collection.add(\n",
    "            embeddings=embeddings.tolist(),\n",
    "            documents=documents,\n",
    "            metadatas=metadatas,\n",
    "            ids=ids\n",
    "        )\n",
    "        \n",
    "        print(f\"‚úÖ {len(documents)} documents ajout√©s avec succ√®s!\\n\")\n",
    "    \n",
    "    def search_similar(self, query: str, n_results: int = 3) -> Dict:\n",
    "        \"\"\"\n",
    "        Recherche les documents les plus similaires √† la requ√™te\n",
    "        \n",
    "        Args:\n",
    "            query: Question de l'utilisateur\n",
    "            n_results: Nombre de documents √† retourner\n",
    "            \n",
    "        Returns:\n",
    "            Dictionnaire contenant documents, distances et m√©tadonn√©es\n",
    "        \"\"\"\n",
    "        # G√©n√©rer l'embedding de la requ√™te\n",
    "        query_embedding = self.embedding_model.encode(\n",
    "            [query],\n",
    "            convert_to_numpy=True\n",
    "        )\n",
    "        \n",
    "        # Recherche dans ChromaDB\n",
    "        results = self.collection.query(\n",
    "            query_embeddings=query_embedding.tolist(),\n",
    "            n_results=n_results\n",
    "        )\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def generate_answer_local(self, query: str, context: str) -> str:\n",
    "        \"\"\"\n",
    "        G√©n√®re une r√©ponse avec Ollama (local)\n",
    "        \"\"\"\n",
    "        prompt = f\"\"\"Tu es un assistant IA qui r√©pond aux questions en te basant sur le contexte fourni.\n",
    "\n",
    "Contexte:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Instructions:\n",
    "- R√©ponds uniquement en te basant sur le contexte fourni\n",
    "- Si la r√©ponse n'est pas dans le contexte, dis-le clairement\n",
    "- Sois pr√©cis et concis\n",
    "- Cite les sources si possible\n",
    "\n",
    "R√©ponse:\"\"\"\n",
    "\n",
    "        payload = {\n",
    "            \"model\": self.llm_model,\n",
    "            \"prompt\": prompt,\n",
    "            \"stream\": False,\n",
    "            \"options\": {\n",
    "                \"temperature\": 0.3,\n",
    "                \"num_predict\": 512\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            response = requests.post(self.llm_url, json=payload, timeout=60)\n",
    "            response.raise_for_status()\n",
    "            return response.json()[\"response\"]\n",
    "        except Exception as e:\n",
    "            return f\"‚ùå Erreur lors de la g√©n√©ration: {str(e)}\"\n",
    "    \n",
    "    def generate_answer_groq(self, query: str, context: str) -> str:\n",
    "        \"\"\"\n",
    "        G√©n√®re une r√©ponse avec Groq API\n",
    "        \"\"\"\n",
    "        if not self.groq_api_key:\n",
    "            return \"‚ùå GROQ_API_KEY non d√©finie. Cr√©ez un fichier .env avec votre cl√© API.\"\n",
    "        \n",
    "        url = \"https://api.groq.com/openai/v1/chat/completions\"\n",
    "        headers = {\n",
    "            \"Authorization\": f\"Bearer {self.groq_api_key}\",\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        }\n",
    "        \n",
    "        payload = {\n",
    "            \"model\": self.llm_model,\n",
    "            \"messages\": [\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"Tu es un assistant qui r√©pond aux questions en te basant uniquement sur le contexte fourni.\"\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"Contexte:\\n{context}\\n\\nQuestion: {query}\"\n",
    "                }\n",
    "            ],\n",
    "            \"temperature\": 0.3,\n",
    "            \"max_tokens\": 512\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            response = requests.post(url, headers=headers, json=payload, timeout=30)\n",
    "            response.raise_for_status()\n",
    "            return response.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "        except Exception as e:\n",
    "            return f\"‚ùå Erreur Groq: {str(e)}\"\n",
    "    \n",
    "    def query(self, question: str, n_results: int = 3) -> Dict:\n",
    "        \"\"\"\n",
    "        Pipeline RAG complet: Question ‚Üí Recherche ‚Üí G√©n√©ration\n",
    "        \n",
    "        Args:\n",
    "            question: Question de l'utilisateur\n",
    "            n_results: Nombre de documents √† utiliser comme contexte\n",
    "            \n",
    "        Returns:\n",
    "            Dictionnaire avec la r√©ponse et les sources\n",
    "        \"\"\"\n",
    "        print(f\"‚ùì Question: {question}\\n\")\n",
    "        \n",
    "        # 1. Recherche vectorielle\n",
    "        print(\"üîç Recherche des documents pertinents...\")\n",
    "        search_results = self.search_similar(question, n_results)\n",
    "        \n",
    "        if not search_results['documents'][0]:\n",
    "            return {\n",
    "                \"answer\": \"Aucun document pertinent trouv√© dans la base.\",\n",
    "                \"sources\": []\n",
    "            }\n",
    "        \n",
    "        # 2. Pr√©parer le contexte\n",
    "        documents = search_results['documents'][0]\n",
    "        metadatas = search_results['metadatas'][0]\n",
    "        distances = search_results['distances'][0]\n",
    "        \n",
    "        context = \"\\n\\n---\\n\\n\".join([\n",
    "            f\"[Document {i+1}] (Similarit√©: {1-dist:.2%})\\n{doc}\"\n",
    "            for i, (doc, dist) in enumerate(zip(documents, distances))\n",
    "        ])\n",
    "        \n",
    "        print(f\"üìö {len(documents)} documents trouv√©s\\n\")\n",
    "        \n",
    "        # 3. G√©n√©ration de la r√©ponse\n",
    "        print(\"ü§ñ G√©n√©ration de la r√©ponse...\")\n",
    "        if self.use_local_llm:\n",
    "            answer = self.generate_answer_local(question, context)\n",
    "        else:\n",
    "            answer = self.generate_answer_groq(question, context)\n",
    "        \n",
    "        # 4. Pr√©parer les sources\n",
    "        sources = [\n",
    "            {\n",
    "                \"content\": doc[:200] + \"...\" if len(doc) > 200 else doc,\n",
    "                \"metadata\": meta,\n",
    "                \"similarity\": f\"{(1-dist)*100:.1f}%\"\n",
    "            }\n",
    "            for doc, meta, dist in zip(documents, metadatas, distances)\n",
    "        ]\n",
    "        \n",
    "        return {\n",
    "            \"answer\": answer,\n",
    "            \"sources\": sources\n",
    "        }\n",
    "    \n",
    "    def reset_database(self):\n",
    "        \"\"\"R√©initialise la base de donn√©es vectorielle\"\"\"\n",
    "        self.chroma_client.reset()\n",
    "        self.collection = self.chroma_client.get_or_create_collection(\n",
    "            name=\"documents\",\n",
    "            metadata={\"hnsw:space\": \"cosine\"}\n",
    "        )\n",
    "        print(\"üóëÔ∏è Base de donn√©es r√©initialis√©e\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# EXEMPLE D'UTILISATION\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialiser le syst√®me RAG\n",
    "    # use_local_llm=True pour Ollama, False pour Groq\n",
    "    rag = RAGSystem(use_local_llm=True)\n",
    "    \n",
    "    # Documents exemples (remplacez par vos propres donn√©es)\n",
    "    documents = [\n",
    "        \"\"\"L'intelligence artificielle (IA) est l'ensemble des th√©ories et des techniques \n",
    "        mises en ≈ìuvre en vue de r√©aliser des machines capables de simuler l'intelligence humaine. \n",
    "        Elle regroupe des domaines comme l'apprentissage automatique, le traitement du langage \n",
    "        naturel et la vision par ordinateur.\"\"\",\n",
    "        \n",
    "        \"\"\"Le RAG (Retrieval-Augmented Generation) est une technique qui combine la recherche \n",
    "        d'informations et la g√©n√©ration de texte. Il permet aux mod√®les de langage d'acc√©der \n",
    "        √† des bases de connaissances externes pour fournir des r√©ponses plus pr√©cises et √† jour.\"\"\",\n",
    "        \n",
    "        \"\"\"ChromaDB est une base de donn√©es vectorielle open source optimis√©e pour les embeddings. \n",
    "        Elle permet de stocker et rechercher des vecteurs de haute dimension de mani√®re efficace, \n",
    "        ce qui la rend id√©ale pour les applications d'IA comme le RAG.\"\"\",\n",
    "        \n",
    "        \"\"\"Mistral est un mod√®le de langage open source d√©velopp√© par Mistral AI en France. \n",
    "        Il offre d'excellentes performances en fran√ßais et peut √™tre ex√©cut√© localement via \n",
    "        des outils comme Ollama, sans n√©cessiter de connexion internet.\"\"\"\n",
    "    ]\n",
    "    \n",
    "    metadatas = [\n",
    "        {\"source\": \"wikipedia\", \"topic\": \"IA\"},\n",
    "        {\"source\": \"documentation\", \"topic\": \"RAG\"},\n",
    "        {\"source\": \"documentation\", \"topic\": \"ChromaDB\"},\n",
    "        {\"source\": \"blog\", \"topic\": \"Mistral\"}\n",
    "    ]\n",
    "    \n",
    "    # Ajouter les documents\n",
    "    rag.add_documents(documents, metadatas)\n",
    "    \n",
    "    # Poser des questions\n",
    "    print(\"=\"*60)\n",
    "    print(\"D√âMONSTRATION DU SYST√àME RAG\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "    \n",
    "    questions = [\n",
    "        \"Qu'est-ce que le RAG ?\",\n",
    "        \"Pourquoi utiliser ChromaDB ?\",\n",
    "        \"Quelles sont les capacit√©s de Mistral ?\"\n",
    "    ]\n",
    "    \n",
    "    for i, question in enumerate(questions, 1):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"QUESTION {i}\")\n",
    "        print('='*60)\n",
    "        \n",
    "        result = rag.query(question, n_results=2)\n",
    "        \n",
    "        print(f\"\\nüí° R√âPONSE:\\n{result['answer']}\\n\")\n",
    "        \n",
    "        print(\"üìñ SOURCES UTILIS√âES:\")\n",
    "        for j, source in enumerate(result['sources'], 1):\n",
    "            print(f\"\\n  [{j}] Similarit√©: {source['similarity']}\")\n",
    "            print(f\"      Sujet: {source['metadata']['topic']}\")\n",
    "            print(f\"      Extrait: {source['content'][:100]}...\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
