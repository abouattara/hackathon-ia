{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89ecdf51",
   "metadata": {},
   "source": [
    "# Systeme de RAG open source complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b2085341",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "SystÃ¨me RAG 100% Open Source\n",
    "Embeddings: sentence-transformers\n",
    "VectorDB: ChromaDB\n",
    "LLM: Mistral via Ollama (ou Groq)\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from typing import List, Dict\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import requests\n",
    "import json\n",
    "\n",
    "class RAGSystem:\n",
    "    def __init__(self, use_local_llm=True):\n",
    "        \"\"\"\n",
    "        Initialise le systÃ¨me RAG\n",
    "        \n",
    "        Args:\n",
    "            use_local_llm: True pour Ollama local, False pour API Groq\n",
    "        \"\"\"\n",
    "        print(\"ğŸš€ Initialisation du systÃ¨me RAG...\")\n",
    "        \n",
    "        # 1. ModÃ¨le d'embeddings (multilingue, lÃ©ger)\n",
    "        print(\"ğŸ“Š Chargement du modÃ¨le d'embeddings...\")\n",
    "        self.embedding_model = SentenceTransformer(\n",
    "            'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'\n",
    "        )\n",
    "        \n",
    "        # 2. Base de donnÃ©es vectorielle ChromaDB\n",
    "        print(\"ğŸ’¾ Initialisation de ChromaDB...\")\n",
    "        self.chroma_client = chromadb.Client(Settings(\n",
    "            anonymized_telemetry=False,\n",
    "            allow_reset=True\n",
    "        ))\n",
    "        \n",
    "        # CrÃ©er ou rÃ©cupÃ©rer une collection\n",
    "        self.collection = self.chroma_client.get_or_create_collection(\n",
    "            name=\"documents\",\n",
    "            metadata={\"hnsw:space\": \"cosine\"}  # SimilaritÃ© cosinus\n",
    "        )\n",
    "        \n",
    "        # 3. Configuration du LLM\n",
    "        self.use_local_llm = use_local_llm\n",
    "        if use_local_llm:\n",
    "            self.llm_url = \"http://localhost:11434/api/generate\"\n",
    "            self.llm_model = \"mistral\"\n",
    "            print(\"ğŸ¤– LLM: Mistral (Ollama local)\")\n",
    "        else:\n",
    "            # Pour utiliser Groq, dÃ©finir GROQ_API_KEY dans .env\n",
    "            self.groq_api_key = os.getenv(\"GROQ_API_KEY\", \"\")\n",
    "            self.llm_model = \"llama3-8b-8192\"\n",
    "            print(\"ğŸ¤– LLM: Groq API\")\n",
    "        \n",
    "        print(\"âœ… SystÃ¨me RAG initialisÃ© avec succÃ¨s!\\n\")\n",
    "    \n",
    "    def add_documents(self, documents: List[str], metadatas: List[Dict] = None):\n",
    "        \"\"\"\n",
    "        Ajoute des documents Ã  la base vectorielle\n",
    "        \n",
    "        Args:\n",
    "            documents: Liste de textes Ã  indexer\n",
    "            metadatas: MÃ©tadonnÃ©es optionnelles pour chaque document\n",
    "        \"\"\"\n",
    "        print(f\"ğŸ“ Ajout de {len(documents)} documents...\")\n",
    "        \n",
    "        # GÃ©nÃ©rer les embeddings\n",
    "        embeddings = self.embedding_model.encode(\n",
    "            documents,\n",
    "            show_progress_bar=True,\n",
    "            convert_to_numpy=True\n",
    "        )\n",
    "        \n",
    "        # CrÃ©er des IDs uniques\n",
    "        ids = [f\"doc_{i}\" for i in range(len(documents))]\n",
    "        \n",
    "        # PrÃ©parer les mÃ©tadonnÃ©es si non fournies\n",
    "        if metadatas is None:\n",
    "            metadatas = [{\"source\": f\"document_{i}\"} for i in range(len(documents))]\n",
    "        \n",
    "        # Ajouter Ã  ChromaDB\n",
    "        self.collection.add(\n",
    "            embeddings=embeddings.tolist(),\n",
    "            documents=documents,\n",
    "            metadatas=metadatas,\n",
    "            ids=ids\n",
    "        )\n",
    "        \n",
    "        print(f\"âœ… {len(documents)} documents ajoutÃ©s avec succÃ¨s!\\n\")\n",
    "    \n",
    "    def search_similar(self, query: str, n_results: int = 3) -> Dict:\n",
    "        \"\"\"\n",
    "        Recherche les documents les plus similaires Ã  la requÃªte\n",
    "        \n",
    "        Args:\n",
    "            query: Question de l'utilisateur\n",
    "            n_results: Nombre de documents Ã  retourner\n",
    "            \n",
    "        Returns:\n",
    "            Dictionnaire contenant documents, distances et mÃ©tadonnÃ©es\n",
    "        \"\"\"\n",
    "        # GÃ©nÃ©rer l'embedding de la requÃªte\n",
    "        query_embedding = self.embedding_model.encode(\n",
    "            [query],\n",
    "            convert_to_numpy=True\n",
    "        )\n",
    "        \n",
    "        # Recherche dans ChromaDB\n",
    "        results = self.collection.query(\n",
    "            query_embeddings=query_embedding.tolist(),\n",
    "            n_results=n_results\n",
    "        )\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def generate_answer_local(self, query: str, context: str) -> str:\n",
    "        \"\"\"\n",
    "        GÃ©nÃ¨re une rÃ©ponse avec Ollama (local)\n",
    "        \"\"\"\n",
    "        prompt = f\"\"\"Tu es un assistant IA qui rÃ©pond aux questions en te basant sur le contexte fourni.\n",
    "\n",
    "Contexte:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Instructions:\n",
    "- RÃ©ponds uniquement en te basant sur le contexte fourni\n",
    "- Si la rÃ©ponse n'est pas dans le contexte, dis-le clairement\n",
    "- Sois prÃ©cis et concis\n",
    "- Cite les sources si possible\n",
    "\n",
    "RÃ©ponse:\"\"\"\n",
    "\n",
    "        payload = {\n",
    "            \"model\": self.llm_model,\n",
    "            \"prompt\": prompt,\n",
    "            \"stream\": False,\n",
    "            \"options\": {\n",
    "                \"temperature\": 0.3,\n",
    "                \"num_predict\": 512\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            response = requests.post(self.llm_url, json=payload, timeout=60)\n",
    "            response.raise_for_status()\n",
    "            return response.json()[\"response\"]\n",
    "        except Exception as e:\n",
    "            return f\"âŒ Erreur lors de la gÃ©nÃ©ration: {str(e)}\"\n",
    "    \n",
    "    def generate_answer_groq(self, query: str, context: str) -> str:\n",
    "        \"\"\"\n",
    "        GÃ©nÃ¨re une rÃ©ponse avec Groq API\n",
    "        \"\"\"\n",
    "        if not self.groq_api_key:\n",
    "            return \"âŒ GROQ_API_KEY non dÃ©finie. CrÃ©ez un fichier .env avec votre clÃ© API.\"\n",
    "        \n",
    "        url = \"https://api.groq.com/openai/v1/chat/completions\"\n",
    "        headers = {\n",
    "            \"Authorization\": f\"Bearer {self.groq_api_key}\",\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        }\n",
    "        \n",
    "        payload = {\n",
    "            \"model\": self.llm_model,\n",
    "            \"messages\": [\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"Tu es un assistant qui rÃ©pond aux questions en te basant uniquement sur le contexte fourni.\"\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"Contexte:\\n{context}\\n\\nQuestion: {query}\"\n",
    "                }\n",
    "            ],\n",
    "            \"temperature\": 0.3,\n",
    "            \"max_tokens\": 512\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            response = requests.post(url, headers=headers, json=payload, timeout=30)\n",
    "            response.raise_for_status()\n",
    "            return response.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "        except Exception as e:\n",
    "            return f\"âŒ Erreur Groq: {str(e)}\"\n",
    "    \n",
    "    def query(self, question: str, n_results: int = 3) -> Dict:\n",
    "        \"\"\"\n",
    "        Pipeline RAG complet: Question â†’ Recherche â†’ GÃ©nÃ©ration\n",
    "        \n",
    "        Args:\n",
    "            question: Question de l'utilisateur\n",
    "            n_results: Nombre de documents Ã  utiliser comme contexte\n",
    "            \n",
    "        Returns:\n",
    "            Dictionnaire avec la rÃ©ponse et les sources\n",
    "        \"\"\"\n",
    "        print(f\"â“ Question: {question}\\n\")\n",
    "        \n",
    "        # 1. Recherche vectorielle\n",
    "        print(\"ğŸ” Recherche des documents pertinents...\")\n",
    "        search_results = self.search_similar(question, n_results)\n",
    "        \n",
    "        if not search_results['documents'][0]:\n",
    "            return {\n",
    "                \"answer\": \"Aucun document pertinent trouvÃ© dans la base.\",\n",
    "                \"sources\": []\n",
    "            }\n",
    "        \n",
    "        # 2. PrÃ©parer le contexte\n",
    "        documents = search_results['documents'][0]\n",
    "        metadatas = search_results['metadatas'][0]\n",
    "        distances = search_results['distances'][0]\n",
    "        \n",
    "        context = \"\\n\\n---\\n\\n\".join([\n",
    "            f\"[Document {i+1}] (SimilaritÃ©: {1-dist:.2%})\\n{doc}\"\n",
    "            for i, (doc, dist) in enumerate(zip(documents, distances))\n",
    "        ])\n",
    "        \n",
    "        print(f\"ğŸ“š {len(documents)} documents trouvÃ©s\\n\")\n",
    "        \n",
    "        # 3. GÃ©nÃ©ration de la rÃ©ponse\n",
    "        print(\"ğŸ¤– GÃ©nÃ©ration de la rÃ©ponse...\")\n",
    "        if self.use_local_llm:\n",
    "            answer = self.generate_answer_local(question, context)\n",
    "        else:\n",
    "            answer = self.generate_answer_groq(question, context)\n",
    "        \n",
    "        # 4. PrÃ©parer les sources\n",
    "        sources = [\n",
    "            {\n",
    "                \"content\": doc[:200] + \"...\" if len(doc) > 200 else doc,\n",
    "                \"metadata\": meta,\n",
    "                \"similarity\": f\"{(1-dist)*100:.1f}%\"\n",
    "            }\n",
    "            for doc, meta, dist in zip(documents, metadatas, distances)\n",
    "        ]\n",
    "        \n",
    "        return {\n",
    "            \"answer\": answer,\n",
    "            \"sources\": sources\n",
    "        }\n",
    "    \n",
    "    def reset_database(self):\n",
    "        \"\"\"RÃ©initialise la base de donnÃ©es vectorielle\"\"\"\n",
    "        self.chroma_client.reset()\n",
    "        self.collection = self.chroma_client.get_or_create_collection(\n",
    "            name=\"documents\",\n",
    "            metadata={\"hnsw:space\": \"cosine\"}\n",
    "        )\n",
    "        print(\"ğŸ—‘ï¸ Base de donnÃ©es rÃ©initialisÃ©e\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "95c5fb9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Initialisation du systÃ¨me RAG...\n",
      "ğŸ“Š Chargement du modÃ¨le d'embeddings...\n",
      "ğŸ’¾ Initialisation de ChromaDB...\n",
      "ğŸ¤– LLM: Mistral (Ollama local)\n",
      "âœ… SystÃ¨me RAG initialisÃ© avec succÃ¨s!\n",
      "\n",
      "ğŸ“ Ajout de 4 documents...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6ce19eb765f4358b0eaf29af337d97c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… 4 documents ajoutÃ©s avec succÃ¨s!\n",
      "\n",
      "============================================================\n",
      "DÃ‰MONSTRATION DU SYSTÃˆME RAG\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "QUESTION 1\n",
      "============================================================\n",
      "â“ Question: Qu'est-ce que le RAG ?\n",
      "\n",
      "ğŸ” Recherche des documents pertinents...\n",
      "ğŸ“š 2 documents trouvÃ©s\n",
      "\n",
      "ğŸ¤– GÃ©nÃ©ration de la rÃ©ponse...\n",
      "\n",
      "ğŸ’¡ RÃ‰PONSE:\n",
      "âŒ Erreur lors de la gÃ©nÃ©ration: 404 Client Error: Not Found for url: http://localhost:11434/api/generate\n",
      "\n",
      "ğŸ“– SOURCES UTILISÃ‰ES:\n",
      "\n",
      "  [1] SimilaritÃ©: 45.6%\n",
      "      Sujet: RAG\n",
      "      Extrait: Le RAG (Retrieval-Augmented Generation) est une technique qui combine la recherche \n",
      "        d'inform...\n",
      "\n",
      "  [2] SimilaritÃ©: 19.5%\n",
      "      Sujet: ChromaDB\n",
      "      Extrait: ChromaDB est une base de donnÃ©es vectorielle open source optimisÃ©e pour les embeddings. \n",
      "        Ell...\n",
      "\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "QUESTION 2\n",
      "============================================================\n",
      "â“ Question: Pourquoi utiliser ChromaDB ?\n",
      "\n",
      "ğŸ” Recherche des documents pertinents...\n",
      "ğŸ“š 2 documents trouvÃ©s\n",
      "\n",
      "ğŸ¤– GÃ©nÃ©ration de la rÃ©ponse...\n",
      "\n",
      "ğŸ’¡ RÃ‰PONSE:\n",
      "âŒ Erreur lors de la gÃ©nÃ©ration: 404 Client Error: Not Found for url: http://localhost:11434/api/generate\n",
      "\n",
      "ğŸ“– SOURCES UTILISÃ‰ES:\n",
      "\n",
      "  [1] SimilaritÃ©: 63.6%\n",
      "      Sujet: ChromaDB\n",
      "      Extrait: ChromaDB est une base de donnÃ©es vectorielle open source optimisÃ©e pour les embeddings. \n",
      "        Ell...\n",
      "\n",
      "  [2] SimilaritÃ©: 25.1%\n",
      "      Sujet: RAG\n",
      "      Extrait: Le RAG (Retrieval-Augmented Generation) est une technique qui combine la recherche \n",
      "        d'inform...\n",
      "\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "QUESTION 3\n",
      "============================================================\n",
      "â“ Question: Quelles sont les capacitÃ©s de Mistral ?\n",
      "\n",
      "ğŸ” Recherche des documents pertinents...\n",
      "ğŸ“š 2 documents trouvÃ©s\n",
      "\n",
      "ğŸ¤– GÃ©nÃ©ration de la rÃ©ponse...\n",
      "\n",
      "ğŸ’¡ RÃ‰PONSE:\n",
      "âŒ Erreur lors de la gÃ©nÃ©ration: 404 Client Error: Not Found for url: http://localhost:11434/api/generate\n",
      "\n",
      "ğŸ“– SOURCES UTILISÃ‰ES:\n",
      "\n",
      "  [1] SimilaritÃ©: 60.8%\n",
      "      Sujet: Mistral\n",
      "      Extrait: Mistral est un modÃ¨le de langage open source dÃ©veloppÃ© par Mistral AI en France. \n",
      "        Il offre d...\n",
      "\n",
      "  [2] SimilaritÃ©: 16.8%\n",
      "      Sujet: IA\n",
      "      Extrait: L'intelligence artificielle (IA) est l'ensemble des thÃ©ories et des techniques \n",
      "        mises en Å“uv...\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# EXEMPLE D'UTILISATION\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialiser le systÃ¨me RAG\n",
    "    # use_local_llm=True pour Ollama, False pour Groq\n",
    "    rag = RAGSystem(use_local_llm=True)\n",
    "    \n",
    "    # Documents exemples (remplacez par vos propres donnÃ©es)\n",
    "    documents = [\n",
    "        \"\"\"L'intelligence artificielle (IA) est l'ensemble des thÃ©ories et des techniques \n",
    "        mises en Å“uvre en vue de rÃ©aliser des machines capables de simuler l'intelligence humaine. \n",
    "        Elle regroupe des domaines comme l'apprentissage automatique, le traitement du langage \n",
    "        naturel et la vision par ordinateur.\"\"\",\n",
    "        \n",
    "        \"\"\"Le RAG (Retrieval-Augmented Generation) est une technique qui combine la recherche \n",
    "        d'informations et la gÃ©nÃ©ration de texte. Il permet aux modÃ¨les de langage d'accÃ©der \n",
    "        Ã  des bases de connaissances externes pour fournir des rÃ©ponses plus prÃ©cises et Ã  jour.\"\"\",\n",
    "        \n",
    "        \"\"\"ChromaDB est une base de donnÃ©es vectorielle open source optimisÃ©e pour les embeddings. \n",
    "        Elle permet de stocker et rechercher des vecteurs de haute dimension de maniÃ¨re efficace, \n",
    "        ce qui la rend idÃ©ale pour les applications d'IA comme le RAG.\"\"\",\n",
    "        \n",
    "        \"\"\"Mistral est un modÃ¨le de langage open source dÃ©veloppÃ© par Mistral AI en France. \n",
    "        Il offre d'excellentes performances en franÃ§ais et peut Ãªtre exÃ©cutÃ© localement via \n",
    "        des outils comme Ollama, sans nÃ©cessiter de connexion internet.\"\"\"\n",
    "    ]\n",
    "    \n",
    "    metadatas = [\n",
    "        {\"source\": \"wikipedia\", \"topic\": \"IA\"},\n",
    "        {\"source\": \"documentation\", \"topic\": \"RAG\"},\n",
    "        {\"source\": \"documentation\", \"topic\": \"ChromaDB\"},\n",
    "        {\"source\": \"blog\", \"topic\": \"Mistral\"}\n",
    "    ]\n",
    "    \n",
    "    # Ajouter les documents\n",
    "    rag.add_documents(documents, metadatas)\n",
    "    \n",
    "    # Poser des questions\n",
    "    print(\"=\"*60)\n",
    "    print(\"DÃ‰MONSTRATION DU SYSTÃˆME RAG\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "    \n",
    "    questions = [\n",
    "        \"Qu'est-ce que le RAG ?\",\n",
    "        \"Pourquoi utiliser ChromaDB ?\",\n",
    "        \"Quelles sont les capacitÃ©s de Mistral ?\"\n",
    "    ]\n",
    "    \n",
    "    for i, question in enumerate(questions, 1):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"QUESTION {i}\")\n",
    "        print('='*60)\n",
    "        \n",
    "        result = rag.query(question, n_results=2)\n",
    "        \n",
    "        print(f\"\\nğŸ’¡ RÃ‰PONSE:\\n{result['answer']}\\n\")\n",
    "        \n",
    "        print(\"ğŸ“– SOURCES UTILISÃ‰ES:\")\n",
    "        for j, source in enumerate(result['sources'], 1):\n",
    "            print(f\"\\n  [{j}] SimilaritÃ©: {source['similarity']}\")\n",
    "            print(f\"      Sujet: {source['metadata']['topic']}\")\n",
    "            print(f\"      Extrait: {source['content'][:100]}...\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
